[{"win label": 0, "news": [{"title": "Scaling the unknown", "date": "2018-07-11", "content": "The next supercomputer frontier presents a journey into the unknown unlike any other, Tzanio Kolev says. Exascale computers, the first of which are expected to begin operation in 2021, will perform a quintillion (a billion billion, or 1018) calculations per second, about eight times faster than Summit, the most powerful supercomputer in the world today. These machines will affect nearly every aspect of research and development, including climate modeling, combustion, additive manufacturing, subsurface flow, wind energy and the design and commercialization of small-module nuclear reactors. But \"nobody has ever built such machines,\" says Kolev, director of the Center for Efficient Exascale Discretizations (CEED) at Department of Energy's (DOE) Lawrence Livermore National Laboratory (LLNL). \"The exascale environment is so complex, it requires the combined efforts of application and software developers and hardware vendors to get there.\" The center is part of the DOE's Exascale Computing Project (ECP), a coordinated effort to prepare for exascale's arrival. CEED's interdisciplinary process, known as co-design, is central to the ECP's approach to ensure the exascale ecosystem it develops can meet the challenges a range of DOE applications present. Moving to exascale supercomputing will require a different way of doing things. Moore's Law, which said computers double in power every year or two, is no longer valid. \"The only way to increase the amount of computation we can perform is by increasing parallelism,\" in which computer processors work cooperatively to solve problems, Kolev says. \"And not just increasing it but making it work on heterogeneous hardware having multiple levels of memory that is much more complex than we're used to.\" CEED is one of five co-design centers in the Exascale Computing Project, a collaboration between DOE's Office of Science and National Nuclear Security Administration. The centers facilitate cooperation between the ECP's supercomputer vendors, application scientists and hardware and software specialists. The CEED collaboration alone encompasses more than 30 researchers at two DOE national laboratories - LLNL and Argonne - and five universities. \"We have a really amazing team,\" Kolev says. \"One of our achievements is how well we work together. That doesn't always happen with such multi-institutional teams.\" Team members grapple with a multitude of tradeoffs involved in designing and producing next-generation supercomputer hardware and software - the heart of co-design. One compromise, for example, is the amount of memory a computer will have and how fast that memory can be accessed. The size, speed and number of memory layers are dictated by algorithms that will use them and how those codes are expected to perform on a given hardware configuration. At a higher level, exascale computers also must hold down fixed costs and energy consumption. \"These machines can easily consume enormous amounts of power,\" Kolev says. CEED focuses on high-order finite-element discretization algorithms, a way of dividing up big problems that could increase performance by orders of magnitude over traditional methods. \"High-order is really a family of mathematically sophisticated algorithms that have actually been around for a long time, but they've always been considered very expensive,\" Kolev says, requiring too many floating-point operations (those involving numbers with fractions) to make them worthwhile. \"Well, now things have changed with the advances in computer architectures.\" Memory access has become the main brake on computer speed. \"The time-to-solution performance that you see is no longer limited by how many floating-point operations you perform in your application. What really matters is how much memory traffic you have, and so, all these floating-point operations that were criticized before actually become an advantage nowadays. They allow you to fully utilize the hardware.\" Low-order methods entail \"a quick computation because there's very little to compute, then you're waiting and waiting to get the next data from memory,\" Kolev explains. In contrast, high-order methods perform many computations on the data brought from memory, potentially achieving optimal performance and leading to fast, efficient and accurate simulations on modern hardware, such as multicore processors and general-purpose graphics processing units (GPUs). Among CEED's contributions is NekCEM/Nek5000, an open-source simulation software package that delivers highly accurate solutions for a spectrum of scientific applications, including electromagnetics, quantum optics, fluid flow, thermal convection, combustion and magnetohydrodynamics. The code received a 2016 R&D 100 Award from R&D magazine as one of the year's top new technologies. And Kolev's team recently issued its CEED 1.0 software distribution, which bundles the 13 code packages the center is developing. Most of them connect to libCEED, the center's new API, or application program interface, library. The API allows programmers to easily access CEED's high-performance algorithms from a variety of applications. CEED researchers are improving the MFEM C++ software library, a main component of the center's applications and finite-element focus areas, by improving algorithms for high-order finite element methods and developing optimized kernels for high-performance hardware, such as GPUs. The co-design center also produced Laghos, the first ever mini-application, or miniapp, for high-order compressible Lagrangian flow, a fluid-flow phenomenon relevant to LLNL and DOE research. The miniapp gives the CEED team a way to share concrete, simplified physics codes with vendors in a relatively controlled setting, garnering feedback to improve performance in the full application. Using such simplified proxies for large-scale codes is central to co-design, as it enables vendors, computer scientists and mathematicians to focus on a big application's essential performance characteristics without diving into complicated physics. The Lagrangian miniapp simulates the movement of fluids or gases at high speeds and under extreme pressures in a computational mesh that moves with the flow. Using high-order methods lets the Laghos meshes deform with the fluid, representing the development of complex interactions such as a vortex or a shear - an impossible task for low-order meshes. \"With high-order mesh elements, the whole element could take a very complex shape, follow the flow, still be in the Lagrangian frame and not tangle, not self-intersect,\" Kolev says. \"This allows us to push these types of Lagrangian simulations much further than it was possible in the past.\" Kolev expects the miniapp to benefit computer simulation of experiments at the LLNL-based National Ignition Facility, which is working to achieve nuclear fusion in the laboratory. Fusion, the process that powers the sun, could provide a revolutionary, nearly limitless energy source. CEED's work has already had a significant impact on several applications, including MARBL, a LLNL code to simulate high energy density physics at the exascale, says Robert Rieben, who leads the project. Such simulations support the Stockpile Stewardship Program, which maintains the safety, security and reliability of the U.S. nuclear deterrent without full-scale testing. MARBL is built on the MFEM mathematical discretization framework, and Laghos serves as a proxy for one of MARBL's components. \"CEED's main contributions to the project are the performance optimizations it enables in the MFEM library, the development and maintenance of the Laghos miniapp, and algorithmic advancements in finite element methods that enable high-performance computing (being) integrated into the MARBL code,\" Kolev says.", "url": "https://deixismagazine.org/2018/07/scaling-the-unknown/"}, {"title": "TACC Wins Next NSF-funded Major Supercomputer", "date": "2018-07-11", "content": "The Texas Advanced Computing Center (TACC) has won the next NSF-funded big supercomputer beating out rivals including the National Center for Supercomputing Applications (NCSA)/University of Illinois at Urbana-Champaign, the San Diego Supercomputer Center, and the Pittsburgh Supercomputing Center among others. The NSF award - Towards a Leadership-Class Computing Facility - Phase 1 - is worth roughly $60 million for the system and another $60 million for five years of operations. A Phase 2 effort for building a true Leadership Class system is called out in the solicitation but with few details and no specific funding. The list of submitters is not made public. Details of the planned TACC system or winning proposal were not immediately available. TACC is delaying public announcements until the \"final contracts are signed\" according to a spokesperson. However, a publicly-available memorandum from the National Science Board (NSF's governing body) reported that NSB passed a resolution last Tuesday \"authorizing the Director to make an award to the Texas Advanced Computing Center at the University of Texas at Austin for the acquisition of the system described in the [TACC] proposal Computation for the Endless Frontier.\" HPCwire will report the details of the new machine when they are available. In describing the project's requirements the NSF solicitation used the Blue Waters (NCSA) supercomputer as a benchmark saying the Phase 1 system should have \"least two to three-fold time-to-solution performance improvement over the UIUC's Blue Waters system for a broad range of computational and data-intensive workflows that require the highest capabilities in terms of scale, throughput, and data analytics.\" It will be fascinating to see who the prime vendors are and what architecture is planned. Blue Waters, headquartered at NCSA, is a 13 petaflops (peak) system built by Cray and powered by AMD Opteron processors and Nvidia K20 GPUs. By comparison to the just-announced award, Blue Waters cost about $190 million capital and roughly $150 million for operations for five years. Reached for comment on the NSF award, Bill Gropp, director of NCSA, congratulated TACC and expressed disappointment for NCSA. He declined to offer details of the NCSA proposal as there has been speculation regarding potential NSF support for an additional system; NCSA would be interested in competing for it if indeed such an opportunity arises and is keeping its cards close to its vest for the moment. Said Gropp, \"The call for the Blue Waters replacement emphasized performance across a wide range of applications. That's a very challenging target. We worked hard to propose a system that would deliver sustained performance in the requested quantity for a variety of applications. Going forward it's going to get harder and harder to do that because the architectures become more specialized in order to do performance targets.\" The new award is sure to spark debate over NSF's selection process and its overall strategy for growing the U.S. HPC infrastructure serving academic science. While the Department of Energy (DoE) has a clear exascale computing program to serve DoE missions, NSF does not. Gropp says, \"NSF essentially doesn't like using the word exascale. To some extent I am ok with that because it is an arbitrary performance number and we should be focused on enabling science.\" Indeed DoE has likewise de-emphasized exaflops capability as the goal and instead has set 50x performance above existing leadership class systems as the benchmark. Many in the HPC community say NSF plans for building extreme scale computing infrastructure in support of science are inadequate. While acknowledging the many pressures NSF faces, Gropp is nonetheless firmly in the camp that believes NSF's plans should be more aggressive. \"The current NSF situation is disappointing. The system that TACC won, if you look at the title of this [NSF solicitation], it is 'Towards Leadership Computing,' which is really truth in advertising, because that's not going to be a leadership system unless TACC got a really great deal from somebody. It's going to be not in the same ballpark as leadership systems either at DoE or elsewhere in the world,\" said Gropp, who was co-chairman of the National Academies Report, Future Directions for NSF Advanced Computing Infrastructure to Support U.S. Science and Engineering in 2017-2020, which supported more aggressive expansion of high end HPC resources. \"The challenge that we see going forward is the level of funding and level of resources provided are declining both pretty much in absolute terms and certainly relative to our international competitors. If you look at the plans that Japan has for what I would consider their Track 2 systems, I think that right now there are 11 systems on that roadmap. That is a real roadmap with specific kinds of systems, performance targets, centers of operations and timelines. Now it's a roadmap; it is not a guarantee, but it shows thinking that allows science teams to plan on what sort of systems are going to be available and what they need to do to adapt to use those systems,\" said Gropp. Gropp noted a roadmap slide presented by NSF at SC16 included second system. At SC17, a similar slide lacked the second system. \"If you read the RFP for this solicitation, it is a two-phase solicitation, but only the first phase is funded. So part of what you had to respond to in the proposal was how you were going to plan for phase two. So another question is how much money they are going to set aside for phase two. Are they prepared to set aside $200- or $300 million dollars for capital and something on the same order for operations, which is what a leadership system costs these days.\" No doubt there will be heightened discussion of NSF's strategy following the TACC award as well as excitement around learning the details of TACC's planned new machine.", "url": "https://www.hpcwire.com/2018/07/30/tacc-wins-next-nsf-funded-major-supercomputer/"}, {"title": "New Texas Supercomputer to Push the Frontiers of Science", "date": "2018-08-29", "content": "The National Science Foundation (NSF) announced today that it has awarded $60 million to the Texas Advanced Computing Center (TACC) at The University of Texas at Austin for the acquisition and deployment of a new supercomputer that will be the fastest at any U.S. university and among the most powerful in the world. The new system, known as Frontera (Spanish for \"frontier\"), will begin operations in 2019. It will allow the nation's academic researchers to make important discoveries in all fields of science, from astrophysics to zoology, and further establishes The University of Texas at Austin's leadership in advanced computing. \"Supercomputers - like telescopes for astronomy or particle accelerators for physics - are essential research instruments that are needed to answer questions that can't be explored in the lab or in the field,\" said Dan Stanzione, TACC executive director. \"Our previous systems have enabled major discoveries, from the confirmation of gravitational wave detections by the Laser Interferometer Gravitational-wave Observatory to the development of artificial-intelligence-enabled tumor detection systems. Frontera will help science and engineering advance even further.\" \"For over three decades, NSF has been a leader in providing the computing resources our nation's researchers need to accelerate innovation,\" said NSF Director France C\u00f3rdova. \"Keeping the U.S. at the forefront of advanced computing capabilities and providing researchers across the country access to those resources are key elements in maintaining our status as a global leader in research and education. This award is an investment in the entire U.S. research ecosystem that will enable leap-ahead discoveries.\" Frontera is the latest in a string of successful awards and deployments by TACC with support from NSF. Since 2006, TACC has built and operated three supercomputers that debuted in the Top 10 most powerful systems in the world: Ranger (2008), Stampede1 (2012) and Stampede2 (2017). Three other systems debuted in the Top 25. If completed today, Frontera would be the fifth most powerful system in the world, the third fastest in the U.S. and the largest at any university. For comparison, Frontera will be roughly twice as powerful as Stampede2 (currently the fastest university supercomputer), and 70 times faster than Ranger, which operated until 2013. To match what Frontera will compute in just one second, a person would have to perform one calculation every second for roughly one billion years. \"Today's NSF award solidifies the University of Texas' reputation as the nation's leader in academic supercomputing,\" said Gregory L. Fenves, president of UT Austin. \"UT is proud to serve the research community with the world-class capabilities of TACC, and we are excited to contribute to the many discoveries Frontera will enable.\" Anticipated early projects on Frontera include analyses of particle collisions from the Large Hadron Collider, global climate modeling, improved hurricane forecasting and multi-messenger astronomy. The primary computing system will be provided by Dell EMC and powered by Intel processors. Data Direct Networks will contribute the primary storage system, and Mellanox will provide the high-performance interconnect for the machine. NVIDIA, GRC (Green Revolution Cooling), and the cloud providers Amazon, Google, and Microsoft will also have roles in the project. Faculty at the Institute for Computational Engineering and Sciences (ICES) at UT Austin will lead the world-class science applications and technology team, with partners from the California Institute of Technology, Cornell University, Princeton University, Stanford University, the University of Chicago, the University of Utah, and the University of California, Davis. Experienced technologists and operations partners from the sites above as well as The Ohio State University, the Georgia Institute of Technology, and Texas A&M University will ensure the system runs effectively in all areas, including security, user engagement and workforce development. Frontera's name alludes to \"Science the Endless Frontier,\" the title of a 1945 report to President Truman by Vannevar Bush that led to the creation of the National Science Foundation. \"NSF was born out of World War II and the idea that science, and scientists, had enabled our nation to win the war, and continued innovation would be required to 'win the peace',\" said Stanzione. \"Many of the frontiers of research today can only be advanced by computing, and Frontera will be an important tool to solve grand challenges that will improve our nation's health, well-being, competitiveness and security.\" Frontera will enter production in the summer of 2019 and will operate for five years. In addition to serving as a resource for the nation's scientists and engineers, the award will support efforts to test and demonstrate the feasibility of an even larger future leadership-class system, ten times faster than Frontera, to potentially be deployed as Phase 2 of the project.", "url": "https://news.utexas.edu/2018/08/29/new-texas-supercomputer-to-push-the-frontiers-of-science/"}, {"title": "Immersion GPU System Provides AI Horsepower for Frontera", "date": "2018-09-04", "content": "What might the rise of artificial intelligence revolution look like in the data center? If one new ssytem is any indication, it could look like GPUs immersed in dielectric liquid coolant fluid, supporting water-cooled x86 servers. That's the vision put forward by the creators of Frontera, a new $60 million supercomputer to be built at the Texas Advanced Computing Center (TACC) in Austin. It is expected to be the most powerful supercomputer at any U.S. university, and continue the TACC's history of deploying new systems ranking among the top 10 on the Top500 list of the world's leading supercomputers. The vast majority of data centers continue to cool IT equipment using air, while liquid coolinghas been used primarily in high-performance computing (HPC). With the growing use of artificial intelligence, more companies are facing data-crunching challenges that resemble those seen by the HPC sector, which could make liquid cooling relevant for a larger pool of data center operators. The design for Frontera reflects the leading edge of HPC efficiency. Frontera is Spanish for \"frontier,\" and the new supercomputer will help advance the frontiers of liquid cooling, with a hybrid system that will combine Dell EMC servers with x86 Intel processors and water-cooling systems from CoolIT, and a smaller system using NVIDIA GPUs (graphic processing units) immersed in a tank of liquid coolant from GRC (previously Green Revolution Cooling). Data Direct Networks will contribute the primary storage system, and Mellanox will provide the high-performance interconnect for Frontera. Applying Immersion Benefits to GPUs: Anticipated early projects for Frontera include analyses of particle collisions from the Large Hadron Collider, global climate modeling, and improved hurricane forecasting and \"multi-messenger\" astronomy research using gravitational waves and electromagnetic radiation. \"Many of the frontiers of research today can be advanced only by computing, and Frontera will be an important tool to solve grand challenges that will improve our nation's health, well-being, competitiveness and security.\" said Dan Stanzione, TACC executive director. TACC has been a leader in the use of immersion cooling, which sinks servers in liquid to cool the components, and began working with Austin-based neighbor GRC in 2009. In 2017 this collaboration was expanded to immersion cooling for NVIDIA GPUs, test-driving a system created by server vendor Supermicro. Using immersion cooling with GPUs is a fairly recent phenomenon, but may attract interest as more companies adopt GPUs for AI and other parallel processing challenges. \"The cost savings that immersion cooling enables (on the hardware side) are extremely impressive,\" TACC's Stanzione said of the 2017 project. \"Being early adopters of GRC's immersion cooling system we have seen the technology mature rapidly over the years. And with the growing power and computing needs of AI and machine learning applications, especially with hotter and hotter GPUs, cooling is even more important for reliability.\" AI Data Crunching Boosts Density: New hardware for AI workloads is packing more computing power into each piece of equipment, boosting the power density - the amount of electricity used by servers and storage in a rack or cabinet - and the accompanying heat. The trend is challenging traditional practices in data center cooling, and prompting data center operators to adapt new strategies and designs. The alternative is to bring liquids into the server chassis to cool chips and components. Some vendors integrate water cooling into the rear-door of a rack or cabinet. This can also be done by immersing servers in tanks of coolant, or through enclosed systems featuring pipes and plates that bring cooling inside the chassis and directly to the processor. The latter approach will be used by the CPU-powered component of Frontera, which will features a CoolIT DLC system adapted for the Dell EMC servers. CoolIT recently shared an image on its social channels showing what a prototype of the Frontera system will look like. \"The new Frontera systems represents the next phase in the long-term relationship between TACC and Dell EMC, focused on applying the latest technical innovation to truly enable human potential,\" said Thierry Pellegrino, vice president of Dell EMC High Performance Computing. \"The substantial power and scale of this new system will help researchers from Austin and across the U.S. harness the power of technology to spawn new discoveries and advancements in science and technology for years to come.\" \"Accelerating scientific discovery lies at the foundation of the TACC's mission, and enabling technologies to advance these discoveries and innovations is a key focus for Intel,\" said Patricia Damkroger, Vice President in Intel's Data Center Group and General Manager, Extreme Computing Group. \"We are proud that the close partnership we have built with TACC will continue with TACC's selection of next-generation Intel Xeon Scalable processors as the compute engine for their flagship Frontera system.\"", "url": "https://milldampr.com/2018/09/20/data-center-frontier-writes-on-the-use-of-grcs-immersion-cooling-technology-in-frontera-supercomputer/"}]}, {"win label": 0, "news": [{"title": "AMD, Cray to build 1.5 exaFlop Frontier supercomputer for Oak Ridge National Lab", "date": "2019-05-07", "content": "The U.S. Department of Energy, along with Oak Ridge National Laboratory, AMD and Cray, announced plans for a more than 1.5 exaFlop supercomputer named Frontier. The $600 million exascale system -- built on AMD's EPYC CPUs and Radeon Instinct GPU processors, as well as Cray's Shasta architecture and Slingshot network -- is set to become the world's fastest supercomputer when it arrives at the lab in 2021. In a press briefing, Oak Ridge lab director Thomas Zacharia said Frontier will be most powerful AI machine anyone has ever seen, with every component purpose built for supercomputing performance. Once live, Frontier will be used to research and model the underlying science of weather, sub-atomic molecular structures, genomics, and physics. AMD chief executive Lisa Su said both the CPUs and GPUs were fully optimized for supercomputing and AI using a future version of its Zen architecture. \"The custom fabric between the CPU and GPU allows for the most efficient communication between these devices,\" Su said. Cray CEO Pete Ungaro said Frontier will be more powerful than the top fastest supercomputers combined. He described an exaFlop as the equivalent to a quintillion calculations per second. In more basic terms, if everyone on Earth carried out one calculation per second, it would take more than six years to perform as many calculations as Frontier will be able to in a single second, said Ungaro. For scale, Ungaro said Frontier will weigh as much as 35 school buses and be the size of two full basketball courts. Frontier will compete against a $500 million, 1 exaFlop supercomputer called Aurora set to arrive in 2021 at Argonne National Laboratory in Chicago. Aurora will include Intel's Xeon Scalable processor as well as the company's compute architecture, persistent memory and software. Cray contributes its Shasta architecture as well as more than 200 cabinets and Slingshot, its scalable interconnect. Tech analyst Patrick Moorhead notes that AMD's supercomputing win is a big deal for the company. \"It has been years since AMD was inside the No.1 supercomputer, all the way back to the Opteron days,\" Moorhead said. \"With Frontier, not only are EPYC CPUs involved, but Radeon Instinct GPUs, too. This bodes well for AMD's future as this is technology that should be in the mainstream market after 2021. It also shows that the ROCm software stack has some legs.\"", "url": "https://www.zdnet.com/article/amd-cray-to-build-1-5-exaflop-frontier-supercomputer-for-oak-ridge-national-lab/"}, {"title": "AMD claims its Frontier supercomputer will hit 1.5 exaflops, making it the world's fastest", "date": "2019-05-07", "content": "AMD today announced that it will partner with Cray to build Frontier, a supercomputer capable of \"exascale\" performance - one that can complete at least a quintillion floating point computations per second, where a flop equals two 15-digit numbers multiplied together - for weather system simulation, subatomic particle modeling, and more. The two companies expect it will be the world's fastest supercomputer when it's delivered in 2021, with more than 1.5 exaflops of theoretical performance - roughly 50 times the speed of today's top supercomputers and faster than the top 160 combined. Frontier will be built at Oak Ridge National Laboratory in Oak Ridge, Tennessee. \"AMD is proud to partner with Cray and Oak Ridge National Laboratory to deliver what is expected to be the world's most powerful supercomputer,\" said Forrest Norrod, SVP and GM of AMD's datacenter and embedded systems group. \"Frontier will feature custom CPU and GPU technology from AMD and represents the latest achievement on a long list of technology innovations AMD has contributed to the Department of Energy exascale programs.\" During a dial-in conference with members of the press, AMD threw around some eye-popping figures. Frontier's network bandwidth will be 24 million times greater than the average home internet connection, or speedy enough to download 100,000 HD movies in a second. The system will have a physical footprint spanning 7,300 square feet, the equivalent of two basketball courts. And Frontier's internal cable and wiring would run all the way from Philadelphia to New York City if laid out flat end-to-end. Driving Frontier's breakthrough compute is what AMD claims is the first \"fully optimized\" GPU and CPU design for supercomputing. It features a custom AMD Epyc processor packing a future Zen core architecture designed for high- performance computing and AI workloads, along with a graphics processing unit in AMD's Radeon Instinct product lineup of server accelerators. The GPUs feature HPC engines, \"extensive\" mixed precision operations, and high- bandwidth memory, and they're linked together - one Epyc processor to four Instinct graphics cards - by AMD's Infinity Fabric and Cray Slingshot high- bandwidth system interconnect architectures. Beyond AMD's bespoke graphics and processor combo, Frontier will incorporate Cray's containerized Shasta software for monitoring, orchestration, adaptive routing, quality-of-service, and congestion management. Moreover, the company says it will architect a high-efficiency direct liquid cooling solution for Frontier and with a separate joint contract will pursue \"new ... technologies,\" including a high-density compute infrastructure and enhancements to its HPC developer tools for GPU scaling and AI. Cray says these forthcoming tools, which will be codeveloped with AMD, will take advantage of AMD's Radeon Open Compute Platform to enable direct communication between Frontier's network interface cards and GPU memory. And Cray says that Cray Programming Environment, the company's eponymous software development suite for HPC apps, will be integrated with a machine learning software stack that will offer support for \"the most popular tools and frameworks.\" \"We are excited to work with the team at AMD to deliver the Frontier system to Oak Ridge National Laboratory,\" said Cray SVP and CTO Steve Scott. \"Cray's Shasta supercomputers are designed to support leading-edge processor technologies and high-performance storage, all tightly interconnected by Cray's ... Slingshot network. The combination of Cray and AMD technology in the Frontier system will dramatically enhance performance at scale for AI, analytics, and simulation, enabling DOE to further push the boundaries of scientific discovery.\" As work progresses on Frontier, Cray and Oak Ridge will establish a Center of Excellence at the lab to \"drive collaboration and innovation\" and assist in the porting and tuning of Department of Energy apps and libraries. Chiefly, the center will be responsible for modernizing new and legacy code and providing training and hands-on workshops. \"Frontier represents the state of the art in high-performance computing. Designing and standing up a machine of its scope requires working closely with industry, partnerships that not only enable breakthrough science but also ensure American scientific and economic competitiveness on the global stage,\" said Oak Ridge's associate laboratory director for computing and computational sciences, Jeff Nichols. \"We are delighted to work with AMD to integrate the CPU and GPU technologies that enable this extremely capable accelerated node architecture.\" Frontier is an outgrowth of the Energy Department's Exascale Computing Project, a grant program within its long-running PathForward initiative, which seeks to accelerate research necessary to develop exascale supercomputers in the U.S. Nearly $258 million in funding has been allocated over a three-year contract period starting 2017, and the companies selected to participate - Cray, Intel, Hewlett Packard Enterprise, IBM, and Nvidia, in addition to AMD - were required to supply supplementary financing amounting to at least 40% of their total project cost. More recently, in April the Energy Department opened requests for two exascale systems as part of its CORAL-2 procurement, with a budget ranging from $800 million to $1.2 billion. Frontier is one of three follow-on machines that are part of CORAL-2, the others being El Capitan at Lawrence Livermore National Laboratory in Livermore, California and Intel's Aurora at Argonne National Laboratory in Chicago. AMD says the contract award for Frontier is valued at more than $600 million. The Department of Energy previously awarded $425 million in federal funding to IBM, Nvidia, and other companies to build two supercomputers: one at Oak Ridge and another at Livermore. The current Oak Ridge system - Summit, which will be replaced by Frontier - delivers between 143 and 200 peak petaflops, according to the TOP500 ranking of supercomputer performance, while Livermore's Sequoia cluster tops out at about 20 petaflops. Both Summit and Sierra were built by IBM and pack IBM Power9 processors and Nvidia Tesla V100 accelerator chips, and both consume enormous amounts of power - up to 13MW, in Summit's case. Assuming AMD delivers on its promise, Frontier will be the crown jewel in the U.S.' supercomputer portfolio, but it might not be the most powerful in the world. Three teams in China - in Tianjin, Jinan, and Beijing - are actively competing to build China's first exascale system in the next seven months, and Japan's Post-K exascale computer has a target deployment date of 2020. Currently, the U.S. hosts five of the 10 fastest computers in the world, with China's best - the TaihuLight at the National Supercomputing Center in Wuxi and the Tianhe-2A in Guangzhou - ranking third and fourth, respectively, at roughly 125 peak petaflops and 100 peak petaflops. Cray's Piz Daint sits in fifth, ahead of Trinity at Los Alamos National Laboratory, Fujitsu's AI Bridge Clouding Infrastructure in Japan, and Lenovo's SuperMUC-NG in Germany. The race between China and the U.S. is fierce. In TOP500 rankings, China two years ago surpassed the United States in total number of ranked supercomputers for the first time, with 202 to 143. That trend accelerated the following year; according to the TOP500 fall 2018 report, the number of ranked U.S. supercomputers fell to 108 as China's total climbed to 229. China and the U.S. are followed in the largest number of ranked supercomputers by Japan, which has 31 systems; the U.K., with 20; France with 18; Germany with 17; and Ireland with 12.", "url": "https://venturebeat.com/ai/amd-claims-its-frontier-supercomputer-will-hit-1-5-exaflops-making-it-the-worlds-fastest/"}, {"title": "CAAR ACCEPTING APPLICATION TEAM PROPOSALS FOR FRONTIER SYSTEM", "date": "2019-05-07", "content": "As details about the Frontier supercomputer emerge, the US Department of Energy's (DOE's) Oak Ridge Leadership Computing Facility (OLCF) is seeking partnerships with select applications teams to develop scientific applications for highly effective use on the Frontier system. Through its Center for Accelerated Application Readiness (CAAR), the OLCF will partner with simulation, data-intensive, and machine learning application teams consisting of application core developers and OLCF staff members. The teams will receive technical support from Cray and AMD-Frontier's primary vendors-and have access to multiple early-generation hardware platforms in the run up to the system's 2021 delivery. Frontier, Oak Ridge National Laboratory's first exascale system, will support the DOE Office of Science in its broad science and energy mission, advancing knowledge in critical areas of government, academia, and industry. \"Dating back to 2008 and the deployment of the Titan supercomputer, CAAR has served as a successful model for preparing accelerated applications for one-of-a-kind systems unlike any of their predecessors in terms of size and scale,\" said Bronson Messer, OLCF CAAR director. \"We're looking forward to renewing this program to ensure that the capability to perform exascale science is awaiting Frontier upon its arrival.\" Based on Cray's new Shasta architecture, Frontier will feature AMD EPYC CPU and Radeon Instinct GPU technology and sport a peak performance exceeding 1.5 exaflops. Each node on Frontier will contain one CPU and four GPUs bridged by coherent memory and high-speed AMD Infinity Fabric links between CPUs and GPUs. Internode communication will be facilitated by Cray's Slingshot interconnect. Leading up to the delivery of Frontier, the CAAR application teams will redesign, port and optimize their software to the system's architecture and demonstrate the effectiveness of their applications through a scientific grand-challenge project. CAAR partnership project proposals, accepted now through June 8, will be evaluated by a computational and scientific review conducted by the OLCF. In addition to gauging the scientific merit and acceleration plan of each proposal, the committee will strive to select a mix of computational algorithms and programming approaches representing a broad range of scientific disciplines.", "url": "https://www.olcf.ornl.gov/2019/05/07/frontiercaar/"}]}, {"win label": 0, "news": [{"title": "CAAR PARTNERSHIPS FOR FRONTIER ANNOUNCED", "date": "2019-09-04", "content": "In preparation for the Frontier supercomputer, the US Department of Energy's (DOE's) Oak Ridge Leadership Computing Facility (OLCF) has selected eight research projects to participate in its Center for Accelerated Application Readiness (CAAR) program. Through CAAR, the OLCF will partner with application core developers, vendor partners, and OLCF staff members to optimize scientific applications for exascale performance, ensuring that Frontier will be able to perform large-scale science when it opens to users in 2022. \"CAAR is a collaboration,\" said Judith Hill, group leader for the Scientific Computing Group at the National Center for Computational Sciences (NCCS). \"The application developers tell us what's broken or not available in the current software stack and what compiler and tool features are their priority, and that feedback helps us prioritize efforts with our vendor partners. These teams help set the direction of software development for this machine.\" As part of the CAAR program, teams will receive support from the US Department of Energy's Oak Ridge National Laboratory Cray/AMD Center of Excellence staff, access to early generation hardware platforms, and early access to Frontier itself. Before Frontier's delivery, each team will use Summit to further develop its projects and to establish a figure of merit, a metric determined by each team that will be used to measure some form of quantifiable performance increase. At the end of the CAAR program, the users will once again run their codes, this time on Frontier, with the results examined based on the individual figures of merit. To be selected for the CAAR program, projects must show a high potential for scientific advancement that cannot be achieved on petascale computers like Summit, the OLCF's IBM AC922 system, which is the fastest supercomputer in the world as of June 2019. \"One of the key components of every one of these projects is that they have a problem that only Frontier will enable them to solve,\" said Bronson Messer, director of the CAAR program. \"That's one of the most important criteria for being in CAAR. They're problems that, as recently as 5 years ago, would have seemed impossible. Figuring out what's possible and then putting the flag out one step beyond that isn't the easiest thing in the world, but I think we've assembled a team of people who can do that.\" Using Frontier, the eight teams will research a broad range of topics, from simulating mass outflows from galaxies like the Milky Way to understanding the way viruses like Zika enter host cells in the body. Some of the projects are set to create massive amounts of data and provide the perfect opportunity for achievements in deep learning and machine learning applications. \"With Summit there was a lot of excitement around machine learning and deep learning, but it was like this explosion of possible ideas,\" Messer said. \"Some of the Frontier CAAR projects have very specific, targeted needs where machine learning and deep learning can be brought to bear. The volume of these simulation results means you can't just sit someone down to look at them over and over again, and that's where something like a machine learning or deep learning approach could really help. I think we are starting to find the place where machine learning, deep learning, and artificial intelligence can really help scientific simulation.\" Frontier is slated to be more than 5 times faster than Summit, with an application performance upwards of 1.5EF, or 1.5 billion billion calculations per second. The system will make use of Cray's new Shasta architecture and feature one AMD EPYC CPU and four AMD Radeon Instinct GPUs per node.", "url": "https://www.olcf.ornl.gov/2019/09/04/caar-partnerships-for-frontier-announced/"}, {"title": "Frontier System Will Offer A Computing Power Of 1.5 Exaflops", "date": "2019-09-27", "content": "Supercomputers are all set to take a gigantic leap forward once the exascale era begins in 2021 with the launch of Aurora. However, Aurora has been robbed of its title before it could even begin by Frontier System. Frontier System will offer a power of more than 1.5 exaflops as opposed to barely over 1 exaflop of power of Aurora. Frontier System is undergoing development as part of the US Department of Energy's Exascale Computing Project and shall be created by Cray Inc. by utilizing AMD processors. It will be housed at Oak Ridge National Laboratory. Funny enough, Aurora is coming out of the same collaboration except for the fact that it makes use of Intel processors. As far as processing power goes, an exaflop is basically one quintillion floating point operations per second. Computer scientists have been busy in trying to cross this line and step into the exascale generation ever since the petascale generation started in 2008. Aurora is the very first exascale supercomputer that was unveiled, as you all might remember. However, it might not be the first one to hit the market and will most definitely not be the fastest. Frontier System is going to be 7.5 times more powerful as compared to the current most powerful supercomputer, Summit, that is operating at 200 petaflops - equivalent of 0.2 exaflops. The Frontier System will be created using more than 100 of new Shasta cabinets from Cray with each housing AMD's EPYC CPUs and Radeon Instinct GPUs, each custom-build for computing in exascale. Making use of artificial intelligence, all of this power will be focused on specific scientific projects that require a gigantic amount of number-crunching including the sub-atomic structures, genomics, physics, and weather. Peter Ungaro, president and CEO of Cray, said, 'Frontier will incorporate foundational new technologies from Cray and AMD that will enable the new exascale era - characterized by data-intensive workloads and the convergence of modeling, simulation, analytics, and AI for scientific discovery, engineering and digital transformation.' There are still two years to go before Frontier and Aurora are launched. We can safely assume that the exascale race is only going to get more tough and exciting!", "url": "https://wonderfulengineering.com/frontier-system-will-offer-a-computing-power-of-1-5-exaflops/"}, {"title": "PREVIEWING THE NEW FRONTIER OF HIGH-PERFORMANCE COMPUTING", "date": "2019-10-28", "content": "In the main banquet room of Knoxville, Tennessee's downtown Hilton Hotel, more than 150 scientists from around the world got their first peek at the exascale computing power that will become available for their research projects in two short years. The US Department of Energy's (DOE's) Oak Ridge Leadership Computing Facility (OLCF)-a DOE Office of Science User Facility at Oak Ridge National Laboratory (ORNL)-presented its first Frontier Application Readiness Kick-Off Workshop from October 8 through 10. The OLCF invited teams from its own Center for Accelerated Application Readiness (CAAR) and the Exascale Computing Project (ECP) to attend presentations and talk with the supercomputer's designers-Cray (a Hewlett Packard Enterprise company) and CPU/GPU-maker AMD-to get a head start on preparing their codes for Frontier's launch in 2021. Considering that some of these codes have been in use for decades, 2 years is a comparatively short amount of time to revamp them for a whole new system. Bronson Messer, an ORNL computational scientist who helped organize the event, said the first wave of application teams selected by the OLCF will need to hit the ground running to make sure their research projects can perform efficiently on Frontier by start-up. \"We have had what some might call this obsession with application readiness ever since we first fielded Titan back in 2012-and we actually started preparing that platform well in advance of 2012,\" Messer told the crowd in his introductory pep talk, mic in hand. \"We want scalable, accelerated scientific applications as soon as Frontier hits the floor.\" The assembled scientists didn't seem to mind this sense of urgency about the tasks ahead of them. The OLCF's Frontier represents a big generational leap in computing speed and accuracy given it will be able to solve calculations up to 10 times faster than the current computer leader, the OLCF's IBM AC922 Summit. As the first supercomputer that will be able to attain 1.5 exaflops (exceeding a quintillion calculations per second), Frontier promises to greatly accelerate scientific discovery. James McClure, a computational scientist at Virginia Tech, has been working on OLCF computers since the 27-petaflops Titan went online in 2012. His team's LBPM application is a Lattice Boltzmann-based code that simulates fluid flows in complex geometries, whether they're geological (such as groundwater) or engineered (such as fuel cells). This was not his first readiness workshop. In fact, he said he always looks forward to refashioning his code to run on more powerful architectures. \"Getting high performance is essential for productivity in science,\" McClure said. \"I think it's certainly true that applications that are able to fully leverage the new supercomputers being built are critical to addressing some of the scientific bottlenecks that exist today. To me, it's pretty exciting to be part of that, and I enjoy doing it.\" The application teams will be supported in their preparation efforts by the Frontier Center of Excellence (CoE), a joint organization led by Cray in partnership with AMD and the OLCF. At the workshop, Frontier CoE personnel trained the CAAR and ECP teams on the current programming environment for early access systems. They also made presentations that shared details of AMD's Radeon Instinct GPU hardware and programming as well as Cray's Shasta architecture, including its new Slingshot interconnect. But CoE instructors benefitted from the workshop as well, said Noah Reddell, Cray's Frontier CoE manager. \"The application readiness kickoff workshop was incredibly helpful for me and the rest of the CoE staff,\" Reddell said. \"We used the event to make contact with all of the application teams and begin discussions for future assistance to ready their applications for exascale and the Frontier architecture. This kicked off close collaborative relationships that will continue for the next several years.\" In addition to attending the presentations, scientists had the opportunity to confer with other researchers, breaking off into discussion groups on October 8 and continuing their interchanges over the next few days. For Sunita Chandrasekaran, assistant professor of Computer and Information Sciences at the University of Delaware, the chance to meet other researchers and discuss their work was energizing. \"I believe this was the most beneficial outcome of the workshop,\" said Chandrasekaran, whose team's application code, PIConGPU, studies plasma-based accelerators toward high energy particles, which can be used for applications in radiation therapy for cancer. \"We made connections with teams and members that we know we are going to find to be extremely helpful resources going forward. Dialogues between domain and computer scientists are key to the success of an interdisciplinary project.\" Likewise, McClure said he enjoys interacting with his peers at the workshop, not only to socialize but also for the very practical purpose of learning new things. \"You have the opportunity to see how other groups are solving problems and learn from that, and sometimes there are things you can pick up that you might not find somewhere else,\" McClure said. \"Also, I think it's pretty cool to be around so many different kinds of science-you're exposed to a lot of different kinds of advancements-because, at the end of the day, all of the groups that are doing this kind of work are tied to very high-impact science-use cases and working with some of the best people in the world.\" The application teams will have a lot of work ahead of them in the next 2 years, from formulating quarterly technical plans to adapting their projects to the hardware and software changes that will occur as the Frontier system nears its launch date. Despite the weight of those responsibilities, Messer said he was most impressed by the teams' palpable excitement at this initial readiness workshop. \"I think people were very enthusiastic about learning something about this brand-new machine,\" Messer said. \"It really is going to be one of the world's first exascale machines, so after having heard about exascale for so many years, practitioners in these disciplines are very anxious to get their hands on an exascale machine.\"", "url": "https://www.olcf.ornl.gov/2019/10/28/previewing-the-new-frontier-of-high-performance-computing/"}]}, {"win label": 0, "news": [{"title": "MAKING ROOM FOR FRONTIER", "date": "2020-01-26", "content": "One of the challenges of cutting-edge equipment at a cutting-edge laboratory is that when a problem arises, the answers are never straightforward. As the 2021 delivery date for Frontier - ORNL's next supercomputer - looms closer and closer, the Oak Ridge Leadership Computing Facility is undergoing some major remodeling, including the removal of about 18,000 square feet of equipment. One of the first systems to go was Titan, the OLCF's groundbreaking Cray XK7 supercomputer, which was decommissioned in August after an impressive seven years in service. Although removing Titan freed up about 4,300 square feet of floor space in the data center, it was only the beginning of the long and arduous task of clearing out the future home of Frontier. More than a dozen disk storage cabinets also needed to be removed from the data center that previously housed Titan, and with each cabinet weighing in at about 3,500 pounds, the size of the systems alone made this a daunting task. Complicating the project further were the 860 storage disks contained in each cabinet. Incredibly sensitive to movement, even the smallest shock - a cart's wheels bumping over the tile floor, for example - could damage one of these disks and cause a loss of data. With each delicate cabinet valued at around $500,000 to $1 million, not including the practically invaluable data stored within, it was imperative that ORNL staff find a way to move the equipment as carefully as possible. One method would be to individually remove each storage disk from the cabinets by hand - almost 13,000 in total - before moving the empty cabinet. \"If we chose to depopulate every drawer in order to move the cabinets, each disk would have to be returned specifically to the slot that it came out of,\" says Paul Abston, data center manager. \"Each disk has a home, and it has to go exactly back into that home. That would mean manually depopulating each drawer and placing the disks into a foam array to keep the order correct, and then once the cabinet is moved, you have to take the correct piece of foam for that enclosure and repopulate those.\" Not only would removing each storage disk by hand be a painstaking and time-consuming process, it would also put the sensitive technology at a high risk of getting damaged. \"Each time you handle the disks, since they're delicate to shock, you take a chance on losing some of them in the process,\" Paul says. \"So, we decided it was better to move everything all together.\" With the traditional method of moving individual nodes out of the question, Paul and his team were entering uncharted territory. \"We needed to be resourceful, so I engaged our rigging supervisor, J.P. Biondo,\" Paul says. \"We discussed using air pads to float the cabinets, but we decided that the air pad technology wouldn't work based on how many seams and threshholds were in the floors. Every time you go across one of those, you're going to lose air pressure and your cabinet will set down. Moreover, you would have to be able to continually move your air supply tank and compressor with the cabinets.\" Without the air pads, the cabinets would have to be pushed on a dolly, but the unique challenges posed by the lab environment and the shock-sensitive technology meant that Paul and his team had to take an unusual approach to the cart's design. \"We talked about pneumatic wheels that would absorb the impact when you crossed thresholds and things like that,\" J.P. says. \"The problem was that pneumatic tires would lift the cabinets too high, making them even more top-heavy than they already were. So I kept searching for impact-resistant wheels, and I eventually ran across these wheels that were made specifically for medical equipment.\" The medical-grade wheels seemed to be the perfect solution, but the team ran into yet another problem. These new wheels were much smaller than the pneumatic alternative and, as a result, could only support a fraction of the weight. The solution: upgrading the cart from four wheels to six. Finally, in mid-September, the team had a dolly custom-designed for the problem at hand, constructed almost entirely out of parts already at ORNL. \"There was just nothing out there available that we could use for this project,\" J.P. says. \"And that's why we went with our own design and fabricated everything ourselves.\" In contrast to many ORNL construction projects, the equipment move was designed, tested and carried out entirely by Lab staff. Using internal Lab resources instead of outside contractors meant that the workers involved in the project were intimately familiar with the equipment, the surrounding area and the safety culture at ORNL. \"It's very important as a Laboratory to use the resources we have internally,\" Paul says. \"Whether you want to know about isotopes, magnetic fields, neutron science, nuclear medicine or computer science, there's likely someone at the Lab that can be a resource for that. That's the same way we need to look at the craft organizations. They are a resource here to support research. So, we had a unique problem, and we knew we had unique expertise here at the lab to help us solve that problem.\" \"It's definitely preferable to try to solve problems internally with people who know what it's like to work in a lab-type setting,\" J.P. says. \"When you start dealing with really sensitive equipment like supercomputers, it's important to have a team that knows how to handle projects safely and carefully. Most people don't deal with or don't understand how valuable and unique the equipment here is, but I feel confident in my supervisors and our craft workers.\" After months of extensive testing, the Facilities and Operations team was left with a brand-new piece of equipment, custom designed for the task at hand. \"At the end of the day, we had a unique problem and we came together to find a unique solution,\" Paul says. \"We were able to draw on the expertise of a lot of different people to design, test, and finally implement this dolly, and that is something we did really well.\"", "url": "https://www.olcf.ornl.gov/2020/01/26/making-room-for-frontier/"}, {"title": "AMD announces details of El Capitan Supercomputer with 2 exaflops of compute power", "date": "2020-03-17", "content": "In the summer of 2019 the USA announced plans for its third Exascale Supercomputer, which is the generation beyond Terascale hardware. The Aurora system uses Intel Xeon processors and is due to be running in the Argonne National Laboratory, Illinois in 2021 with performance of 1 Exaflops. Also in 2021 Cray will deliver the Frontier Supercomputer to Oak Ridge which is powered by AMD EPYC to deliver 1.5 Exaflops. Frontier fills a room and uses 100 Cray Shasta cabinets with Slingshot interconnects between the nodes and has a power consumption around 30MW. We now know that El Capitan will be delivered in early 2023 and will also use Cray Shasta cabinets so broadly speaking El Capitan will look like Frontier however the performance will increase to 2 Exaflops with power consumption remaining under 40MW. 2 Exaflops is a whole heap of compute power and yes, we know FLOPS is an acronym for Floating Point Operations per Second so the correct form is ExaFLOPS. In our recent news post about AMD Radeon Pro W5500 graphics we talked about performance up to 5.5 Teraflops which is 5.5x 10 to the 12. By contrast El Capitan will have a claimed performance of 2x 10 to the 18 so we are talking about the equivalent of hundreds of thousands of graphics cards which seems reasonable as the price of El Capitan is quoted as USD $600 million. El Capitan will be installed at the Lawrence Livermore National Laboratory and will be used primarily to model nuclear weapons for the US Department of Energy. AMD tells us \"The system features next generation AMD EPYC processors, codenamed Genoa featuring the Zen 4 processor core, next generation AMD Radeon Instinct GPUs based on a new compute optimised architecture, 3rd Gen AMD Infinity Architecture and open source AMD ROCm heterogeneous computing software\". At present AMD is using its Zen 2 architecture with DDR4 memory and we expect to see Zen 3 later this year or perhaps early in 2021. While Zen 3 is claimed to use a new architecture the current rumours suggest it will continue to use TSMC 7nm fabrication process with the same motherboard sockets and the same DDR4 memory. The change from Zen 2 to Zen 3 is likely to be a reordering of the cache system to share memory and reduce latency. We expected Zen 4 to arrive during 2021 on a revised 7nm process, perhaps using Extreme Ultra Violet and therefore named 7nm+, however it now seems likely Zen 4 will move to TSMC 5nm. Zen 4 will support 'Next Generation Memory' which is presumably DDR5 however we also see the next generation Radeon Instinct GPUs will use 'Next Generation High Bandwidth Memory'. In addition AMD has named its next version of Infinity Fabric as Infinity Architecture which seems to be based on PCI Express Gen. 5 with the ability for one EPYC to communicate with four Radeon Instinct GPUs. AMD tells us Infinity Architecture will support 'Unified memory across CPU and GPU' which seems like a contradiction in terms. Our best guess is that Zen 4 will use DDR5 system memory while the Radeon Instinct GPUs in El Capitan use HBM2E with Infinity Architecture allowing the EPYC CPU in each node to communicate with the graphics memory on the four GPUs for specific AI workloads. The 2 Exaflops headline is impressive however our main interest with El Capitan is the insight we have gained into AMD's plans over the next two or three years. We are impressed by the current Zen 2 architecture and Ryzen 3000 processors however Zen 3 and Zen 4 sound like they will be a real blast and will be coming in the very near future. ", "url": "https://www.redsharknews.com/technology-computing/item/7065-amd-announces-details-of-el-capitan-supercomputer-with-2-exaflops-of-compute-power"}]}, {"win label": 0, "news": [{"title": "ORNL successfully installs 2.5-mile power line for the OLCF's Frontier supercomputer", "date": "2020-09-23", "content": "The US Department of Energy's (DOE's) Oak Ridge National Laboratory (ORNL) is poised to deliver Frontier, its first exascale supercomputer, next year. Exascale systems are capable of operating at 1 quintillion calculations per second, requiring a significant amount of power and generating substantial amounts of heat. Before the engineers working on the Frontier supercomputer could bring new hardware into the building that used to house the Oak Ridge Leadership Computing Facility's (OLCF's) Cray XK7 Titan supercomputer, they first had to install a 2.5-mile-long power line from one of the laboratory's nearby electrical substations all the way to the computer room. \"We put in new power poles and ran additional electrical feeders from the substation,\" said Justin Whitt, project director for Frontier at the OLCF, a DOE Office of Science User Facility at ORNL. \"We needed to provide about 40 megawatts of power and cooling to the Frontier data center.\" The project began last fall with design work and a field survey of the entire right of way-the path of the transmission line-with multiple divisions in ORNL to ensure that wildlife and habitat wouldn't be impacted in the construction process. More than a dozen species of bats reside on the Oak Ridge Reservation today, including an endangered Indiana bat. Most of the bats in Tennessee spend their winters in Tennessee caves and migrate to warmer, forested habitats-like those at the laboratory-in the summer. This restricts construction in the area during the summer months. ORNL plant ecologist Jamie Herold, technical professional Kitty McCracken of the laboratory's Environmental Sciences Division, and clean water act compliance specialist Todd North in the Environmental Protection Services Division all took part in the survey. After the environmental issues were identified and permits were issued, the Roads and Grounds Crew led by ORNL's Matt Powell and Zackary Moore of the Logistical Services Division began carving out roads to prepare for the power line installation. With fencing to be torn down, trees to be cleared, gravel to be spread, and a ridge and lake to be crossed, the project would be no easy feat. \"Roads and Grounds works with our environmental groups and understands the environmental rules and regulations,\" said Jack Wilkinson, a project manager in ORNL's Laboratory Modernization Division. \"November to March is the only time they have to actually clear trees because the bats migrate here in the summer.\" After the roads were installed, Wright Construction completed an approvals process that involved the Tennessee Valley Authority, the Tennessee Department of Environment & Conservation, and the Army Corps of Engineers to ensure environmental and safety regulations were followed during the installation. Then, Wilkinson's team began the daunting task of installing the power line, beginning at the substation. The project was massive. The project also required staff to dig 10-foot-deep trenches in the substation and under roads to install banks of electrical conduit, and the rain the area experienced at the beginning of the year proved to be one of the biggest challenges, Wilkinson said. \"In January and February, we had at least double the amount of rain we normally do,\" Wilkinson said. \"We needed dirt to backfill these trenches, but since it was wet, we couldn't pack it down enough, so we had to fill the trenches with gravel instead.\" Wilkinson applauded the determination of the construction workers in completing the project. The team, overseen by ORNL field construction manager Ken Brown, consistently persevered through muddy, cold, and otherwise unfavorable outdoor conditions to meet project deadlines. The end result was more than 2.5 miles of 13.8-kilovolt electrical cables running along 84 new power poles down Ramsey Drive and Melton Valley Drive and across Haw Ridge. The design of the lines takes into account snowfall, wind conditions, temperature conditions, and the rate of expansion, resulting in the ideal amount of sag and tension on the lines. Multiple access pads across Haw Ridge link the lines straight from Melton Valley Drive to White Oak Avenue and the 5600-5700-5800 complex where the OLCF's supercomputers live. Next on the team's agenda is the transformers that will be installed just outside of the Frontier data center to step down the power. A team will then further distribute the electricity inside of the Frontier data center itself by making connections available for the new system. Throughout the project, safety was at the core of the team's work, said Justin Whitt. Over time, the teams working on the project have developed processes that use electromagnetic scanning and ground-penetrating radar to determine the locations of electrical conduit underneath the floors of buildings. In addition to the power line installation, a team led by ORNL high-performance computing mechanical engineer David Grant also built a new mechanical plant that will provide cooling to the Frontier data center. The team converted existing laboratory space into mechanical space and ran steel piping from the cooling plant to the data center. Frontier's cooling system pushes the boundaries of warm water cooling to save the laboratory on operating costs and is expected to be 30 to 40 percent more efficient than the cooling system for Summit, the OLCF's flagship supercomputer. Because the Frontier cooling system could face load swings of 10 megawatts or larger within minutes, the team working on the project designed it to anticipate near-term heat-load swings. When not in use, the cooling system capacity will cool other data center loads.", "url": "https://www.olcf.ornl.gov/2020/09/23/powering-frontier/"}, {"title": "ORNL Successfully Installs 2.5-mile Power Line for the OLCF's Frontier Supercomputer", "date": "2020-09-28", "content": "The US Department of Energy's (DOE's) Oak Ridge National Laboratory (ORNL) is poised to deliver Frontier, its first exascale supercomputer, next year. Exascale systems are capable of operating at 1 quintillion calculations per second, requiring a significant amount of power and generating substantial amounts of heat. Before the engineers working on the Frontier supercomputer could bring new hardware into the building that used to house the Oak Ridge Leadership Computing Facility's (OLCF's) Cray XK7 Titan supercomputer, they first had to install a 2.5-mile-long power line from one of the laboratory's nearby electrical substations all the way to the computer room. \"We put in new power poles and ran additional electrical feeders from the substation,\" said Justin Whitt, project director for Frontier at the OLCF, a DOE Office of Science User Facility at ORNL. \"We needed to provide about 40 megawatts of power and cooling to the Frontier data center.\" The project began last fall with design work and a field survey of the entire right of way-the path of the transmission line-with multiple divisions in ORNL to ensure that wildlife and habitat wouldn't be impacted in the construction process. More than a dozen species of bats reside on the Oak Ridge Reservation today, including an endangered Indiana bat. Most of the bats in Tennessee spend their winters in Tennessee caves and migrate to warmer, forested habitats-like those at the laboratory-in the summer. This restricts construction in the area during the summer months. ORNL plant ecologist Jamie Herold, technical professional Kitty McCracken of the laboratory's Environmental Sciences Division, and clean water act compliance specialist Todd North in the Environmental Protection Services Division all took part in the survey. After the environmental issues were identified and permits were issued, the Roads and Grounds Crew led by ORNL's Matt Powell and Zackary Moore of the Logistical Services Division began carving out roads to prepare for the power line installation. With fencing to be torn down, trees to be cleared, gravel to be spread, and a ridge and lake to be crossed, the project would be no easy feat. \"Roads and Grounds works with our environmental groups and understands the environmental rules and regulations,\" said Jack Wilkinson, a project manager in ORNL's Laboratory Modernization Division. \"November to March is the only time they have to actually clear trees because the bats migrate here in the summer.\" After the roads were installed, Wright Construction completed an approvals process that involved the Tennessee Valley Authority, the Tennessee Department of Environment & Conservation, and the Army Corps of Engineers to ensure environmental and safety regulations were followed during the installation. Then, Wilkinson's team began the daunting task of installing the power line, beginning at the substation. The project was massive. \"Some of the rolls of wire had about 8,000 feet on them and weighed around 8,800 pounds,\" Wilkinson said. \"We had to run 12 conductors a total of 2.5 miles for about 160,000 feet of large wire overhead. There were about 50 employees on the project scattered throughout the area at any given time.\" The project also required staff to dig 10-foot-deep trenches in the substation and under roads to install banks of electrical conduit, and the rain the area experienced at the beginning of the year proved to be one of the biggest challenges, Wilkinson said. \"In January and February, we had at least double the amount of rain we normally do,\" Wilkinson said. \"We needed dirt to backfill these trenches, but since it was wet, we couldn't pack it down enough, so we had to fill the trenches with gravel instead.\" Wilkinson applauded the determination of the construction workers in completing the project. The team, overseen by ORNL field construction manager Ken Brown, consistently persevered through muddy, cold, and otherwise unfavorable outdoor conditions to meet project deadlines. The end result was more than 2.5 miles of 13.8-kilovolt electrical cables running along 84 new power poles down Ramsey Drive and Melton Valley Drive and across Haw Ridge. The design of the lines takes into account snowfall, wind conditions, temperature conditions, and the rate of expansion, resulting in the ideal amount of sag and tension on the lines. Multiple access pads across Haw Ridge link the lines straight from Melton Valley Drive to White Oak Avenue and the 5600-5700-5800 complex where the OLCF's supercomputers live. Next on the team's agenda is the transformers that will be installed just outside of the Frontier data center to step down the power. A team will then further distribute the electricity inside of the Frontier data center itself by making connections available for the new system. Throughout the project, safety was at the core of the team's work, said Justin Whitt. Over time, the teams working on the project have developed processes that use electromagnetic scanning and ground-penetrating radar to determine the locations of electrical conduit underneath the floors of buildings. \"You don't want to be chipping away at the floor and hit a power line beneath,\" Whitt said. \"The 'as-built' drawings should reflect every power line that's laid, but from a safety standpoint, we want to be sure there's no existing electrical conduit in the area we are about to make penetrations into. If there is any doubt, we have to shut off power to the area, which can be disruptive inside an operating building.\" In addition to the power line installation, a team led by ORNL high-performance computing mechanical engineer David Grant also built a new mechanical plant that will provide cooling to the Frontier data center. The team converted existing laboratory space into mechanical space and ran steel piping from the cooling plant to the data center. Frontier's cooling system pushes the boundaries of warm water cooling to save the laboratory on operating costs and is expected to be 30 to 40 percent more efficient than the cooling system for Summit, the OLCF's flagship supercomputer. Because the Frontier cooling system could face load swings of 10 megawatts or larger within minutes, the team working on the project designed it to anticipate near-term heat-load swings. When not in use, the cooling system capacity will cool other data center loads.", "url": "https://www.hpcwire.com/off-the-wire/ornl-successfully-installs-2-5-mile-power-line-for-the-olcfs-frontier-supercomputer/"}, {"title": "Aurora's Troubles Move Frontier into Pole Exascale Position", "date": "2020-10-01", "content": "Intel's 7nm node delay has raised questions about the status of the Aurora supercomputer that was scheduled to be stood up at Argonne National Laboratory next year. Aurora was in the running to be the United States' first exascale supercomputer although it was on a contemporaneous timeline with Oak Ridge National Lab's Frontier supercomputer (with both systems scheduled for delivery inside of 2021). With a one-year delay of Intel's 7nm node that is integral to Aurora's GPU engine (the Intel Xe-based Ponte Vecchio), would Intel contract an outside foundry to fab the GPU die? And what would the impact be on the speeds and feeds and delivery schedule for Aurora? We don't have all those answers yet, but we did get broad confirmation of the disruption from the DOE's Office of Science. There are indications that Aurora will in fact be delayed, but Frontier at Oak Ridge National Laboratory is on track as is the Exascale Computing Project, reported Barb Helland, associate director of the Office of Science for Advanced Scientific Computing Research (ASCR) during an Advanced Scientific Computing Advisory Committee (ASCAC) meeting, held last week (Sept. 24-25). \"It's not unexpected that when we're entering into contracts for the most advanced supercomputers in the world, four to five years before they're deployed, that there will be some schedule delays,\" said Helland. \"For that reason, we build both cost and schedule contingencies into our project budgets.\" The DOE Office of Science was not ready to provide further details at this time but did state they are working closely with Intel. \"Yes, we have indications that the Aurora system will be delayed. But Argonne is currently working with Intel to mitigate the consequences not only to Argonne, but to the Exascale Computing Project and to the nation's high-performance computing users.\" While seeming to downplay the setback, Helland reiterated that Oak Ridge's Frontier machine is on track to be delivered in the calendar year 2021, and that the ECP project is also on track to complete on time (by Q4 of FY24 at the outside). \"I'm confident we're going to get through this in a way that gets this problem resolved to the to the benefit of the country and the program,\" said Chris Fall, director of the Office of Science. \"We're still having conversations and figuring out the details, but I'm very comfortable. I think we'll get where we need to get on that.\" It is reasonable for systems pushing the boundaries of scope and scale to encounter unforeseen circumstances that impact target goalposts, but Aurora has already been significantly redefined after previous delays and cancellations in Intel's roadmaps. Originally conceived as a pre-exascale supercomputer to be stood up at Argonne in 2018, Aurora was recast in 2017 as the nation's first exascale machine with a 2021 target. It would appear that Oak Ridge National Lab's Frontier supercomputer is now lined up to be the nation's first exascale system. The DOE is working with Oak Ridge, HPE and AMD to stand up the 1.5 exaflops (minimum peak) Frontier in late 2021. Lawrence Livermore Lab's El Capitan system (slated to deliver 2 exaflops peak using HPE and AMD technology) is scheduled for approximately one year later (delivery in early 2023). The question is: where will Aurora fit in the timeline? HPE Cray EX supercomputing is the foundation of all three planned exascale systems. HPE is the prime contractor on Frontier and El Capitan, while Intel is the prime on Aurora. In a statement provided to HPCwire, Intel said it \"remains committed to delivering the Aurora supercomputer to Argonne National Laboratory and enabling exascale leadership at the U.S. Department of Energy.\"", "url": "https://www.hpcwire.com/2020/10/01/auroras-troubles-move-frontier-into-pole-exascale-position/"}, {"title": "ORNL Begins Datacenter Buildout for New Exascale Supercomputer Frontier", "date": "2020-12-15", "content": "When the US Department of Energy's (DOE's) new exascale supercomputer, Frontier, completes installation at Oak Ridge National Laboratory (ORNL) in 2021, it will debut as a landmark in high-performance computing with groundbreaking performance of greater than 1.5 exaflops (one quintillion floating-point operations per second). But right now-long before Frontier's technological advances are made available to the world's scientists-the room it will occupy is undergoing a complete mechanical, electrical, and structural transformation. Frontier will reside in the former data center of the Oak Ridge Leadership Computing Facility's (OLCF's) Cray XK7 Titan supercomputer; once the most powerful supercomputer in the world, it was decommissioned after 7 years of service on August 1 last year. It took about a month for a team of HPE Cray technicians to dismantle 430,000 pounds' worth of Titan components and remove them for recycling. Just days later, work began to revamp the 20,000-square-foot room to accommodate Frontier's much higher requirements for power, cooling, and structural support. That meant everything in room E102 of Building 5600 had to be stripped out: piping, electrical infrastructure, even the floor.\"Titan at peak probably consumed about 10 megawatts of power. At peak, Frontier will consume about 30 megawatts. If you use more power, you have to get rid of additional heat, so we are adding the equivalent of 40 megawatts of cooling capacity, about 11,000 tons, for Frontier-much bigger pipes to distribute cool water to the computer,\" said Justin Whitt, program director for the OLCF, a DOE Office of Science User Facility located at ORNL. \"Additionally, supercomputer systems have become denser and heavier with each new generation, and Frontier is no exception to that, so we upgraded the raised floor so it could support that weight.\" With demolition completed, a total work force of about 100 contractors and ORNL craft employees is currently installing that new infrastructure, welding in serpentine piping both above and below the huge, empty room while simultaneously building out a new raised floor. (Now completed, the floor consists of over 4,500 tiles weighing 48 pounds each-or nearly 110 tons altogether.) Long before the data center's construction began-even before the demo work to gut the room-another massive building project had to be tackled: a central energy plant for all the machinery that will be feeding electricity and cooling to Frontier. The new supercomputer's cooling water towers will have a system volume of 130,000 gallons, with 350-horsepower pumps that can each move over 5,000 gallons per minute of the high-temperature water through the Frontier system. The four pumps will connect to the data center via 500 linear feet of 24-inch pipe. While a new 28-megawatt electrical room for the system's transformers was built around the perimeter of the E102 computer room-taking up what was once the office space of OLCF's leadership group-planners still had to find a big enough area for Frontier's cooling towers and all their associated infrastructure. They ended up going to the building next door, 5800, which could provide the needed space-but also required more moves and had some architectural hurdles to overcome. \"The building that we're putting the mechanical plant into was originally designed as a lab space, with not a lot of structure to it-it was basically just holding up the roof,\" said Bart Hammontree, technical project manager for ORNL's Laboratory Modernization Division. \"But we're putting in the neighborhood of a million pounds' worth of piping and cooling towers on the roof of this building. So we had to basically build a new structure inside of an existing building and we had to put new foundations in to support all that.\" In laymen's terms, this makes for difficult, complicated work. Crews had to saw-cut the entire slab underlying Building 5800, rip it out, and dig new foundations inside the building-while avoiding many electrical conduits passing through the construction area to supply power to other parts of the building. Building 5800 is still an operational space, with labs conducting scientific research even during construction.Although the team has diagrams showing where these power lines ought to be, they decided not to take any chances-and turned to technology for added safety. \"We brought in a specialty consultant that uses ground-penetrating radar and an electromagnetic wand to scan the area looking for signals from live conduits. It went really well once we took the time to do that investigative step,\" Hammontree said. Every large project at ORNL includes a risk management program that predicts potential issues that may slow the work, which helps each team preplan mitigation strategies to stay on schedule and within budget. However, there was one factor that the Frontier team did not anticipate when it was assembling its risk register in 2018: a global pandemic. Taking all the precautions necessary to ensure the team remains healthy has presented challenges. In March, ORNL instituted stringent rules on who it would allow on its campus, requiring COVID screening for all contractors and visitors. \"Especially in the early days, it was very difficult to get contractors from out of state or from outside of East Tennessee in, so we had to do a lot of planning any time we needed to bring in a specialty contractor from out of the area,\" Hammontree said. \"That was very challenging. But the medical staff has been great-they worked with us any time we needed one of our contractors tested and we got the results back really quickly. That has allowed us to keep marching forward.\" A construction project of this scale normally takes about 2 years to complete, Whitt said-but to put Frontier into service as soon as possible, the team plans to complete the work in less than a year and a half. \"The biggest challenge that we face at this point is just the incredibly aggressive schedule that we set for the work,\" Whitt said. \"We're responding to the science need for exascale computers so we had to accelerate the time frame for everything-for the technologies, for having the room ready. So we're doing more work for the OLCF than we've ever done before and we're doing it on a shorter time scale.\" Despite the effects of the pandemic, the team is on track to complete the data center in spring of 2021.", "url": "https://www.hpcwire.com/off-the-wire/ornl-begins-datacenter-buildout-for-new-exascale-supercomputer-frontier/"}]}, {"win label": 0, "news": [{"title": "OLCF Announces Storage Specifications for Frontier Exascale System", "date": "2021-05-20", "content": "A newly enhanced I/O subsystem will support the nation's first exascale supercomputer and the Oak Ridge Leadership Computing Facility (OLCF), a US Department of Energy high-performance computing user facility. The OLCF announced storage specifications for their pioneering HPE Cray Frontier supercomputer, an exascale-class system set to power up by year's end. The computational might of exascale computing, expected to have top speeds of 1 quintillion-that's 1018, or a billion billion-calculations per second, promises to enable breakthrough discoveries across the scientific spectrum when Frontier opens to full user operations in 2022, from the basics of building better nuclear reactors to insights into the origins of the universe. The I/O subsystem will consist of two major components: an in-system storage layer and a center-wide file system. The center-wide file system, called Orion, will use open-source Lustre and ZFS technologies. \"To the best of our knowledge, Orion will be the largest and fastest single file POSIX namespace file system in the world,\" said Sarp Oral, Input/Output Working Group lead for Frontier and the Technology Integration Group leader for the National Center for Computational Sciences Division at the OLCF. \"This system marks the fourth generation in the OLCF's long history of developing and deploying large-scale, center-wide file systems to power exploration of grand-challenge scientific problems.\" Frontier's arrival brings the new Orion file system to the OLCF, home to Summit, the nation's fastest supercomputer, and to predecessors Titan and Jaguar. Orion employs Lustre, an open-source parallel file regimen that supported Titan and Jaguar and continues to support high-performance computing systems around the world. Orion will employ Lustre technologies that include distributed name space, data on metadata, and progressive file layouts. Orion will comprise three tiers: a flash-based performance tier of 5,400 nonvolatile memory express (NVMe) devices providing 11.5 petabytes (PB) of capacity at peak read-write speeds of 10 TBps with more than 2 million random-read IOPS; a hard-disk-based capacity tier of 47,700 perpendicular magnetic recording devices providing 679 PB of capacity at peak read speeds of 5.5 TBps and peak write speeds of 4.6 TBps with more than 2 million random-read IOPS; and a flash-based metadata tier of 480 NVMe devices providing an additional capacity of 10 PB. \"Orion is pushing the envelope of what is possible technically due to its extreme scale and hard disk/NVMe hybrid nature,\" said Dustin Leverman, leader of the OLCF's High-Performance Computing Storage and Archive Group. \"This is a complex system, but our experience and best practices will help us create a resource that allows our users to push science boundaries using Frontier.\" Orion will have 40 Lustre metadata server nodes and 450 Lustre object storage service (OSS) nodes. Each OSS node will provide one object storage target (OST) device for performance and two OST devices for capacity-a total of 1,350 OSTs systemwide. An extra 160 nodes will serve as routers to provide peak read-write speeds of 3.2 terabytes to all other OLCF resources and platforms. The in-system storage layer will employ compute-node local storage devices connected via PCIe Gen4 links to provide peak read speeds of more than 75 terabytes per second (TBps), peak write speeds of more than 35 TBps, and more than 15 billion random-read input/output operations per second (IOPS). OLCF engineers are working on software solutions to provide a distributed per-job name space for the devices.", "url": "https://www.hpcwire.com/off-the-wire/olcf-announces-storage-specifications-for-frontier-exascale-system/"}, {"title": "Frontier Supercomputer to Get World's Fastest Storage: 75 TB/s, 15 Billion IOPS, 700 PetaBytes", "date": "2021-05-23", "content": "The Oak Ridge Leadership Computing Facility (OLCF) has announced the first details about the Orion storage subsystem of its upcoming Frontier exascale supercomputer set to go online in late 2021. Being the industry's first 1.5 ExaFLOPS supercomputer, Frontier will need a very fast storage subsystem. It looks like it is set to get one with up to 700 Petabytes of storage, 75 TB/s of throughput, and 15 billion IOPS (yes, billion) of performance on tap. \"To the best of our knowledge, Orion will be the largest and fastest single file POSIX namespace file system in the world,\" said Sarp Oral, Input/Output Working Group lead for Frontier at OLCF. The Frontier supercomputer will actually have two storage sub-systems: an in-system storage layer featuring massive sequential read performance of over 75TB/s and around 15 billion read IOPS, as well as a center-wide file system called Orion that offers a whopping 700PB of capacity. The Orion Global File Storage System Layer: 700PB Capacity at 10TB/s. Since Frontier relies on HPE's Cray Shasta architecture, its global file storage system will largely rely on the ClusterStor multi-tier architecture that uses both PCIe 4.0/NVMe solid-state drives as well as traditional hard disk drives. The Cray ClusterStor machines use AMD EPYC processors and can automatically align data flows in the file system with the workload and shift I/O operations between different tiers of storage as needed. Such shifting makes applications believe that they are accessing high-performance all-flash arrays, thus maximizing performance. As for the software side of matters, Orion will use an open-source Lustre parallel file system (used by loads of supercomputers worldwide, including OLCF's Titan and Jaguar) as well as ZFS with a volume manager. In general, Frontier's center-wide Orion will have three tiers: A metadata tier comprising of 480 NVMe SSDs with 10PB of capacity. An NVMe storage tier that uses 5,400 SSDs providing 11.5PB of capacity, peak read-write speeds of 10TB/s, and over 2 million random-read input/output operations per second (IOPS). An HDD storage tier based on 47,700 PMR hard drives offering 679PB of capacity, a peak read speed of 5.5TB/s, a peak write speed of 4.6TB/s, and over 2 million random-read IOPS. OLCF says that Orion will have 40 Lustre metadata server nodes and 450 Lustre object storage service (OSS) nodes, a total of 1,350 OSTs systemwide. Each OSS node will provide one object storage target (OST) device for performance and two OST devices for capacity. In addition, Orion will employ 160 nodes for routing that will offer peak read-write speeds of 3.2 TB/s available to other OLCF resources and platforms. \"Orion is pushing the envelope of what is possible technically due to its extreme scale and hard disk/NVMe hybrid nature,\" said Dustin Leverman, leader of the OLCF's High-Performance Computing Storage and Archive Group. \"This is a complex system, but our experience and best practices will help us create a resource that allows our users to push science boundaries using Frontier.\" The In-Storage Layer: Up to 75TB/s at 15 Billion Read IOPS. Frontier's in-storage layer comprises of SSDs installed directly into compute nodes and connected to AMD's EPYC processors using a PCIe Gen 4 interface. These NVMe drives will offer an aggregate performance of over 75TB/s read speed, over 35TB/ write speed, and over 15 billion random-read IOPS. The OLCF did not disclose the capacity of the in-storage layer, but this is just local storage, so do not expect tens of petabytes here. ", "url": "https://www.tomshardware.com/news/olcf-describes-frontier-storage-sub-system"}]}, {"win label": 0, "news": [{"title": "OLCF Programming Environment Team Working to Deliver Frontier's Promised Precision", "date": "2021-07-22", "content": "Oak Ridge National Laboratory's (OLCF) \"Pioneering Frontier\" series features stories profiling the many talented ORNL employees behind the construction and operation of the OLCF's incoming exascale supercomputer, Frontier. The HPE Cray system is scheduled for delivery in 2021, with full user operations in 2022. The world's fastest supercomputer comes with some assembly required. Frontier, the nation's first exascale computing system, won't come together as a whole until all pieces arrive at the US Department of Energy's (DOE's) Oak Ridge National Laboratory to be installed-with the eyes of the world watching-on the data center floor inside the Oak Ridge Leadership Computing Facility (OLCF). Once those components operate in harmony as advertised, David Bernholdt and his team can take time for a quick bow-and then get back to work. Bernholdt and his staff lead the effort to ensure the myriad compilers, performance tools, debuggers, and other elements of the Frontier puzzle fit together to deliver the world-leading results intended. The feeling can be like that of a pit crew fine-tuning a race car-except this car is the first of its kind, and it's headed into the final lap as they work on it. The HPE Cray system promises computational speeds that top 1 quintillion calculations per second-more than 1018, or a billion billion, operations-and could help solve challenges across the scientific spectrum when Frontier opens to full user operations in 2022. \"When DOE started thinking seriously about exascale, back around 2009, we wondered if it was even going to be possible to do this,\" Bernholdt said. \"I always figured we would find a way. Now the technology has evolved to the point that it's not nearly as scary to build and program and debug as we thought it would be, and we're coming down to the finish line. It's been quite a ride.\" Getting Frontier to the finish line for his team includes interacting with users-the scientists and engineers eager to enter their codes and kick off high-speed simulations of everything from weather patterns and stages of cancer to nuclear reactions and collapsing stars-and the corporate vendors working around the clock to meet specifications and deliver a first-of-its-kind product. Because the OLCF encourages standards-based programming environments when feasible, the team also works closely with a variety of standards organizations to ensure those standards reflect the needs of users and maximize support for the vendors' hardware capabilities. \"It's sometimes daunting to remember: This is Serial No. 1,\" Bernholdt said. \"They've created this machine just for us, and even the best hardware and software are not going to be perfect, at least not right out of the box. A large part of what we do is try to understand what users will need from the system to use it effectively and help them represent that in their codes. \"At the same time, we're working with our vendors and compilers to make sure their solutions implement the standards we need and provide the necessary performance on the system. We have to make sure the vendors are providing enough detailed, granular information about the system in a timely enough fashion that the software developers can take advantage of it, and we have to make sure the languages have evolved enough to carry out the tasks. There are a lot of moving parts, and they're only moving faster as we go.\" Bernholdt's scientific background prepared him for the mission. He spent his early research career in computational chemistry and helped develop NWChem, a scalable software package for computational chemistry still used worldwide. Plans call for a revamped version of the package to run on Frontier and other exascale supercomputers, such as Aurora. Bernholdt later moved into computer science and software development to design and refine tools that tackle some of the same problems he encountered as a scientific user. He and his team helped program and debug Frontier's supercomputing predecessors: Titan (27 petaflops, or 27 quadrillion calculations per second) and Summit (200 petaflops, or 200 quadrillion calculations per second). \"This is our third accelerator-based machine, so we have a reasonably good idea of how to program these,\" Bernholdt said. \"The biggest single challenge has been the schedule. We've had maybe half the time to get Frontier ready that we had for Summit, and it's a newer software stack that's kept everybody scrambling. But that means more opportunities and incentives for optimization.\" That 'round-the-clock work won't end when Frontier switches on. Bernholdt and his team will continue monitoring the supercomputer's performance and looking for ways to raise standards and boost performance. \"It never stops,\" Bernholdt said. \"It's always really satisfying to see people able to use the system to good effect, but that's not the ending. Frontier will continue to evolve and improve, and we'll be part of that. I feel pretty confident saying there is no other place on earth right now that could support a similar project of this scale and importance.\"", "url": "https://www.hpcwire.com/off-the-wire/olcf-programming-environment-team-working-to-deliver-frontiers-promised-precision/"}, {"title": "AMD Continues Frontier Exascale Supercomputer Enablement", "date": "2021-08-25", "content": "AMD is building the world's fastest supercomputer, Frontier, which will deliver exascale-class performance for the US Oak Ridge National Laboratory (ORNL). The supercomputer brings a lot of new technologies to the table, and AMD is laying the groundwork for the software stack that will enable the Frontier to run smoothly. As reported by Phoronix, that work continues in the form of newly-submitted Linux kernel patches. The Frontier supercomputer is a $600 million project that aims to provide more than 1.5 ExaFLOPs of computational power that will be used by ORNL for work on various government projects. Using next-generation EPYC processors and Radeon Instinct graphics cards from AMD, this system will bring a combination of novel memory, storage, and processing elements into one system. According to today's Linux kernel patch submitted by AMD, \"AMD is building a system architecture for the Frontier supercomputer with a coherent interconnect between CPUs and GPUs. This hardware architecture allows the CPUs to coherently access GPU device memory. We have hardware in our labs and we are working with our partner HPE on the BIOS, firmware, and software for delivery to the DOE.\" That stands in contrast to Intel's Aurora, which was projected to be the U.S.'s first supercomputer at the time of its announcement. However, that system has now been delayed into the 2022-2023 timeframe, meaning that the AMD-powered Frontier will not only be the fastest exascale-class computer in the world, it will also be the first. The continued code work continues. Back in May, AMD began the work of ensuring proper support for Frontier's leading-edge storage subsystem. Frontier involves one of the first large-scale deployments with GPU-to-CPU memory coherency, which will require additional code work and qualification. As you can see in the patch notes below, today's work advances Frontier's memory management capabilities. \"The system BIOS advertises the GPU device memory (aka VRAM) as SPM (special purpose memory) in the UEFI system address map. The amdgpu driver registers the memory with devmap as MEMORY_DEVICE_PUBLIC using devm_memremap_pages. This patch series adds MEMORY_DEVICE_PUBLIC, which is similar to MEMORY_DEVICE_GENERIC in that it can be mapped for CPU access, but adds support for migrating this memory similar to MEMORY_DEVICE_PRIVATE.\" \"We also included and updated two patches from Ralph Campbell (Nvidia), which change ZONE_DEVICE reference counting as requested in previous reviews of this patch series. Finally, we extended hmm_test to cover migration of MEMORY_DEVICE_PUBLIC. This work is based on HMM and our SVM memory manager, which has landed in Linux 5.14 recently.\"", "url": "https://www.tomshardware.com/news/frontier-linux-kernel-support"}, {"title": "US Closes in on Exascale: Frontier Installation Is Underway", "date": "2021-09-29", "content": "At the Advanced Scientific Computing Advisory Committee (ASCAC) meeting, held by Zoom this week (Sept. 29-30), it was revealed that the Frontier supercomputer is currently being installed at Oak Ridge National Laboratory in Oak Ridge, Tenn. The staff at the Oak Ridge Leadership Computing Facility, backed by the Exascale Computing Project community and technology partner HPE, is hard at work fielding the United States' first exascale system this year, and ensuring that the system is ready for real science on day one. Associate Director of the DOE's Office of Science for Advanced Scientific Computing Research (ASCR) Barb Helland declared that the agency has met its priority goal to \"by September 30, 2021, begin deployment (receiving and installing hardware) of at least one Exascale Computing system.\" This is a major milestone in the DOE's larger mission goal to \"engage in research and development to create a capable exascale computing ecosystem that integrates hardware and software capability delivering at least 100 times the performance of current 10 petaflops (10^15 floating-point operations per second) systems across a range of applications representing government needs and societal priorities such as Artificial Intelligence (Al) technologies.\" \"We're getting cabinets,\" Helland told the nearly 150 ASCAC meeting attendees, \"Frontier is being delivered.\" \"And this is important because the department had an agency priority goal ... to engage in the research and development of a capable exascale computing ecosystem. The main milestone that we had to meet in this agency priority goal is 'by September 30, 2021, begin the deployment, receiving and installing, of at least one exascale system.' That system is Frontier,\" she said. Built by HPE, Frontier will span 9,000+ Cray EX nodes, each consisting of one third-gen AMD Epyc CPU plus four Radeon Instinct MI200 GPUs, connected via Slingshot 11 networking. Frontier is projected to provide more than 1.5 exaflops of HPC and AI processing performance, according to ORNL. This summer, we learned that Frontier is poised to meet the once-aspirational 20MW exascale power target set by DARPA in 2008. Delivering 1.5 exaflops in 29 megawatts comes out to 19.33 megawatts for 1 exaflops. If accomplished, this impressive jump in energy efficiency will showcase the technology advances made by HPE (which acquired Cray in 2019) and AMD, which just today publicized its goal to achieve 30x energy efficiency gains by 2025. These and other technology advances would not have been possible in this timeframe without the public investment delivered through a series of exascale-development programs (FastForward, etc.) and other investments, made via the CORAL (Collaboration of Oak Ridge, Argonne and Livermore) programs as well as the National Strategic Computing Initiative and Exascale Computing Project.", "url": "https://www.hpcwire.com/2021/09/29/us-closes-in-on-exascale-frontier-installation-is-underway/"}, {"title": "Oak Ridge upgrades data center set to be home to the world's first exascale supercomputer, Frontier", "date": "2021-10-01", "content": "The Department of Energy has begun installing what will become the world's most powerful supercomputer. Oak Ridge National Laboratory has finished upgrading the data center that will house Frontier, set to be the world's first exascale system (capable of at least 10^18 floating point operations per second). Frontier is an HPE Cray system featuring 9,000+ Cray EX nodes, each of which has one AMD Epyc CPU (third gen) and four Radeon Instinct MI200 GPUs. It is expected to be capable of a theoretical peak performance of more than 1.5 exaflops, consuming 29MW. \"If you use more power, you have to get rid of additional heat, so we are adding the equivalent of 40MW of cooling capacity, about 11,000 tons of refrigeration, for Frontier-much bigger pipes to distribute cool water to the computer,\" Justin Whitt, program director for the OLCF, a DOE Office of Science User Facility located at ORNL, said last year. The system can be expanded to handle around 70MW, should the supercomputer grow in the coming years. Whitt continued: \"Additionally, supercomputer systems have become denser and heavier with each new generation, and Frontier is no exception to that, so we upgraded the raised floor so it could support that weight.\" Staff installed more than 4,500 floor tiles weighing 48 pounds (21.7kg) each, to support the cabinets that each weigh 8,000 pounds (3,630kg) - in comparison, the Summit supercomputer's cabinets weigh 2,500 pounds (1,130kg). The site also needed two huge cooling towers with a system volume of 130,000 gallons, with 350-horsepower pumps that can each move up to 10,000 gallons per minute of 90\u00b0F (32\u00b0C) water through the Frontier system. The four pumps will connect to the data center via 500 linear feet of 24-inch pipe. The facility was previously home to the Cray XK7 Titan supercomputer, which was sent to be recycled. \"I do not believe we have previously recycled a system as large as Titan,\" said Craig Webb, a senior manager of logistics at HPE who oversees the company's recycling Take-Back Program.", "url": "https://www.datacenterdynamics.com/en/news/oak-ridge-upgrades-data-center-set-to-be-home-to-the-worlds-first-exascale-supercomputer-frontier/"}, {"title": "Installation of Exascale Supercomputer 'Frontier' at Oak Ridge National Lab Now Underway", "date": "2021-12-27", "content": "An exascale supercomputer called Frontier is now being installed and integrated at Oak Ridge National Laboratory's Leadership Computing Facility in Tennessee and some researchers are expected to gain access to the machine in the summer of 2022, Nextgov reported Thursday. \"Some early users will get access to Frontier this summer to help harden the system for full user operations on Jan. 1, 2023,\" Justin Whitt, program director for the Oak Ridge Leadership Computing Facility, told the publication Wednesday. In May 2019, the Department of Energy awarded Cray a potential $600 million contract to build the Frontier supercomputer, which will incorporate technologies from Advanced Micro Devices. In September of the same year, Hewlett Packard Enterprise closed its acquisition of Cray four months after it first announced the deal. Whitt said Frontier will not only provide simulation and modeling capabilities, but will also offer \"unprecedented opportunities to utilize artificial intelligence and machine learning techniques for issues of global importance, like discovering new patterns in patient data for precision medicine uncovering the origins of disease, shedding light on new properties of materials, and advancing research in high-energy physics.\"", "url": "https://executivegov.com/2021/12/installation-of-supercomputer-frontier-at-oak-ridge-national-lab-now-underway/"}]}, {"win label": 1, "news": [{"title": "Ahead of Frontier's Deployment This Year, 1.5 Cabinet 'Crusher' Serves Science", "date": "2022-03-28", "content": "The Frontier supercomputer was installed at Department of Energy's Oak Ridge National Laboratory in 2021, with the final cabinet rolled into place in October. While shakeout of the full 2-exaflops peak system continues - we have heard off-record about troubles with the interconnect technology - the Frontier project is running with a smaller testbed system of the same core design. Clocking in at about 40 petaflops peak double-precision, \"Crusher\" is a 1.5-cabinet iteration of the Cray EX Frontier supercomputer. Crusher will serve early science users while integration and testing of the full 74-cabinet Frontier system continues. The Frontier system is on track to be the United States' first exascale system sometime this year, and will enter full user operations on January 1, 2023, according to Oak Ridge National Laboratory. Crusher consists of 192 HPE Cray EX nodes - each with one AMD \"Trento\" 7A53 Epyc CPU and four AMD Instinct MI250X GPUs (for a total 768 GPUs). Trento uses the same Zen-3 cores as Milan, optimized for better memory efficiencies. Nodes are connected by HPE's Slingshot-11 interconnect. Each node sports 512GiB DDR4 memory on the CPU and 512GiB HMB2e (128GiB per GPU) with coherent memory across the node. By contrast, the full-size Frontier is slated to deliver 2 exaflops of peak double-precision performance in 74 cabinets within a 29MW power envelope. Occupying a 372 m2 footprint at the Oak Ridge Leadership Computing Facility (OLCF), Frontier spans 9,408 nodes aggregating 9.2 petabytes of memory (4.6 petabytes of DDR4 and 4.6 petabytes of HBM2e). Total GPU count: 37,632. There are 37 petabytes of node local storage, and access to 716 petabytes of center-wide storage. The HPE Olympus racks used in the Frontier architecture are entirely liquid-cooled, including the DIMMs and NICs. Each cabinet (when dry) weighs 3,630 kilograms. The full Frontier system has a total of 81,000 cables. Crusher, said Oak Ridge, is ready to \"crush\" science, although we suspect the name might also be a nod to the chief medical officer from the television series Star Trek: The Next Generation. By extension, the full configuration would be the \"Final Frontier.\" Four projects have already had their codes successfully optimized for Crusher and thus Frontier as well. They are the CANcer Distributed Learning Environment, or CANDLE, project; the Computational hydrodynamics on (parallel) architectures, or Cholla, project; the Locally Self-Consistent Multiple Scattering, or LSMS, project; and the Nuclear Coupled-Cluster Oak Ridge, or NuCCOR, project. Some of these codes date back to OLCF's first hybrid-architecture system, the decommissioned 27-petaflop Cray XK7 Titan supercomputer that also employed CPU+GPU nodes and which was stood up in 2012. Highlights of early results: The CANDLE team has successfully run one of their Transformer models (for natural language processing) on Crusher, achieving an 80 percent speedup on a Crusher node from previous systems. Cholla, one of the first astrophysics codes to be rewritten for Frontier, is seeing 15-fold speedups on Crusher. A materials code - LSMS - that can perform large-scale calculations of up to 100,000 atoms has been successfully deployed on Crusher. NuCCOR, a nuclear physics code that can perform massive simulations of nuclei, is seeing 8-fold speedups on Crusher. \"Crusher is the latest in a long line of test and development systems we have deployed for early users of OLCF platforms and is easily the most powerful of these we have ever provided,\" said ORNL's Bronson Messer, OLCF director of science. \"The results these code teams are realizing on the machine are very encouraging as we look toward the dawn of the exascale era with Frontier.\" \"Taking up only 44 square feet of floor space, Crusher is 1/100th the size of the previous Titan supercomputer but faster than the entire 4,352-square-foot system was, packing a massive computing punch for its small size,\" further reported the Oak Ridge announcement. Frontier was originally scheduled to be deployed in the back half of 2021 and accepted in 2022. Delays of some kind or another are typical with supercomputing systems of this scope and scale, and Frontier is the first implementation of the AMD A+A architecture in addition to being one of the world's first exascale machines. It remains to be seen whether Frontier will be ready in time for the late-May (not June this year) Top500 list as had been widely anticipated (given that the system was fully installed prior to the release of the November 2021 list). Oak Ridge did not offer a precise timeline for Frontier's deployment and acceptance other than stating it will happen in 2022, followed by full operations commencing on January 1, 2023. One challenge that Oak Ridge and their vendor partners have already overcome pertains to Covid-spurred supply chain shortages. Speaking at SCA22 earlier this month, ORNL Corporate Research Fellow Al Geist said that of Frontier's 59 million parts, there were about 2 million parts that the regular manufacturers could not supply. \"There was a heroic effort by the HPE and AMD teams calling up electronics warehouses and [...] other manufacturers and [sourcing the missing parts.]\" A leadership-class facility (it's in the name), OLCF is the home of Summit, another heterogeneous CPU-GPU system that debuted in 2018. Delivering 149 Linpack petaflops, the IBM-built machine is currently the number two system on the twice-yearly Top500 list of fastest computers. The title of world's fastest supercomputer is officially held by the Riken Arm-based Fujitsu system (442 petaflops peak), but China is thought to have two exascale systems that were withheld from the list for political reasons. Two other exascale systems are on deck in the United States: Aurora at Argonne National Laboratory and El Capitan at Livermore National Laboratory. Aurora, having had several resets and setbacks, is slated to be stood up at Argonne National Lab later this year. The Intel-HPE collaboration is now targeting more than 2-exaflops peak performance. On the face of it, Frontier's slowed rollout could conceivably put those timelines in contention; however, Frontier is already already on the floor and Aurora isn't. The Ponte Vecchio GPU for the Aurora supercomputer won't be delivered until later this year, Intel recently reported. Meanwhile, preparation for El Capitan is well underway at Livermore; the system - to be built by HPE using a similar architecture as Frontier - is slated for delivery in 2023, promising greater than 2-exaflops peak performance.", "url": "https://www.hpcwire.com/2022/03/28/ahead-of-frontiers-deployment-this-year-1-5-cabinet-crusher-serves-science/"}, {"title": "FRONTIER SUPERCOMPUTER DEBUTS AS WORLD'S FASTEST, BREAKING EXASCALE BARRIER", "date": "2022-05-29", "content": "The Frontier supercomputer at the Department of Energy's Oak Ridge National Laboratory earned the top ranking today as the world's fastest on the 59th TOP500 list, with 1.1 exaflops of performance. The system is the first to achieve an unprecedented level of computing performance known as exascale, a threshold of a quintillion calculations per second. Frontier features a theoretical peak performance of 2 exaflops, or two quintillion calculations per second, making it ten times more powerful than ORNL's Summit system. The system leverages ORNL's extensive expertise in accelerated computing and will enable scientists to develop critically needed technologies for the country's energy, economic and national security, helping researchers address problems of national importance that were impossible to solve just five years ago. \"Frontier is ushering in a new era of exascale computing to solve the world's biggest scientific challenges,\" ORNL Director Thomas Zacharia said. \"This milestone offers just a preview of Frontier's unmatched capability as a tool for scientific discovery. It is the result of more than a decade of collaboration among the national laboratories, academia and private industry, including DOE's Exascale Computing Project, which is deploying the applications, software technologies, hardware and integration necessary to ensure impact at the exascale.\" Rankings were announced at the International Supercomputing Conference 2022 in Hamburg, Germany, which gathers leaders from around the world in the field of high-performance computing, or HPC. Frontier's speeds surpassed those of any other supercomputer in the world, including ORNL's Summit, which is also housed at ORNL's Oak Ridge Leadership Computing Facility, a DOE Office of Science user facility. Frontier, a HPE Cray EX supercomputer, also claimed the number one spot on the Green500 list, which rates energy use and efficiency by commercially available supercomputing systems, with 62.68 gigaflops performance per watt. Frontier rounded out the twice-yearly rankings with the top spot in a newer category, mixed-precision computing, that rates performance in formats commonly used for artificial intelligence, with a performance of 6.88 exaflops. The work to deliver, install and test Frontier began during the COVID-19 pandemic, as shutdowns around the world strained international supply chains. More than 100 members of a public-private team worked around the clock, from sourcing millions of components to ensuring deliveries of system parts on deadline to carefully installing and testing 74 HPE Cray EX supercomputer cabinets, which include more than 9,400 AMD-powered nodes and 90 miles of networking cables. \"When researchers gain access to the fully operational Frontier system later this year, it will mark the culmination of work that began over three years ago involving hundreds of talented people across the Department of Energy and our industry partners at HPE and AMD,\" Jeff Nichols, ORNL associate lab director for computing and computational sciences, said. \"Scientists and engineers from around the world will put these extraordinary computing speeds to work to solve some of the most challenging questions of our era, and many will begin their exploration on Day One.\" Frontier's overall performance of 1.1 exaflops translates to more than one quintillion floating point operations per second, or flops, as measured by the High-Performance Linpack Benchmark test. Each flop represents a possible calculation, such as addition, subtraction, multiplication or division. Frontier's early performance on the Linpack benchmark amounts to more than seven times that of Summit at 148.6 petaflops. Summit continues as an impressive, highly ranked workhorse machine for open science at number four. \"The idea of these benchmarks is to perform these operations on unimaginably large problems, not to do a massive number of disconnected problems that don't have any bearing on each other,\" said Arjun Shankar, head of the OLCF's Advanced Technologies Section. \"These tests show the machine can solve a really large, complicated problem in a short amount of time.\" Frontier's mixed-precision computing performance clocked in at roughly 6.88 exaflops, or more than 6.8 quintillion flops per second, as measured by the High-Performance Linpack-Accelerator Introspection, or HPL-AI, test. The HPL-AI test measures calculation speeds in the computing formats typically used by the machine-learning methods that drive advances in artificial intelligence. Detailed simulations relied on by traditional HPC users to model such phenomena as cancer cells, supernovas, the coronavirus or the atomic structure of elements require 64-bit precision, a computationally demanding form of computing accuracy. Machine-learning algorithms typically require much less precision - sometimes as little as 32-, 24- or 16-bit accuracy - and can take advantage of special hardware in the graphic processing units, or GPUs, relied on by machines like Frontier to reach even faster speeds. \"This type of mixed-precision computing is enabling new scientific applications that can employ these computationally cheaper algorithms,\" said Feiyi Wang, an OLCF research scientist and leader of ORNL's Analytics and AI Methods at Scale Group. ORNL and its partners continue to execute the bring-up of Frontier on schedule. Next steps include continued testing and validation of the system, which remains on track for final acceptance and early science access later in 2022 and open for full science at the beginning of 2023.", "url": "https://www.olcf.ornl.gov/2022/05/29/frontier-supercomputer-debuts-as-worlds-fastest-breaking-exascale-barrier/"}, {"title": "AMD-powered Frontier supercomputer ranked as fastest in the world", "date": "2022-05-30", "content": "The Oak Ridge National Laboratory's Frontier supercomputer has won the top spot on the Top500 list of the world's fastest supercomputers. The latest edition of the Top500 list, which was released today, ranks Frontier as the world's fastest supercomputer with performance of 1.102 exaflops per second. An exaflop equals 1 quintillion, or a million trillion, computing operations per second. Frontier's performance was measured using a popular benchmark test known as HPL. The top spot on the Top500 list was previously held by the Fugaku supercomputer in Kobe, Japan. Fugaku achieved performance of 442 petaflops in a test conducted using the HPL benchmark. However, the system's theoretical top speed is believed to be in excess of one exaflop. The Frontier supercomputer was constructed as part of a technology initiative that the U.S. Department of Energy announced in May 2019. Officials entrusted the task of developing Frontier to Advanced Micro Devices Inc. and Cray Inc., a major supercomputer builder. A few days after the initiative to build Frontier was announced, Hewlett Packard Enterprise Co. inked a deal to acquire Cray for $1.3 billion. The acquisition established HPE as one of the supercomputing market's largest players. The company has since incorporated Cray's products into its supercomputing portfolio. Frontier comprises 74 HPE Cray EX cabinets that each accommodate 128 blades, a type of compact server. A blade usually has fewer components than a standard data center server, which makes it easier to cool. Certain tasks related to hardware maintenance are simpler as well. Each blade in Frontier includes one central processing unit from AMD's Epyc processor line and four Instinct MI250x accelerators. The MI250x accelerator is a computing module sold by AMD that features two graphics processing units made using a six-nanometer manufacturing process. The GPUs are each supported by a significant amount of HBM2E memory, a type of high-speed memory used to store the data being processed by a chip for rapid access. Overall, Frontier includes some 9,408 CPUs and 37,632 GPUs. The heat produced by the chips is dissipated using a specialized cooling system that moves 5,900 gallons of water through Frontier every minute. Water conducts heat better than air, which generally makes water cooling systems more efficient than fans. HPE says that Frontier is not only the world's fastest supercomputer but also the most power-efficient. Frontier provides 52.23 gigaflops of performance per watt, with 1 gigaflop equal to a billion computing operations per second. The chips, cabinets and other components that comprise Frontier are linked together by 90 miles of cables. The system's CPUs and GPUs are supported by a high-speed storage system dubbed Orion that provides 700 petabytes of capacity. Orion can write more than 35 terabytes of data per second to storage, as well as carry out north of 15 billion random-read input/output operations per second. \"Today's debut of the Frontier exascale supercomputer delivers a breakthrough of speed and performance, and will give us the opportunity to answer questions we never knew to ask,\" said Justin Hotard, executive vice president and general manager of HPE's HPC and AI unit. \"Frontier is a first-of-its-kind system that was envisioned by technologists, scientists and researchers to unleash a new level of capability to deliver open science, AI and other breakthroughs, that will benefit humanity.\" According to HPE, Frontier will enable researchers to develop neural networks eight times larger than those that can be created using earlier hardware. Additionally, the company said, AI training will be 4.5 times faster. \"Scientists and engineers from around the world will put these extraordinary computing speeds to work to solve some of the most challenging questions of our era, and many will begin their exploration on Day One,\" said Jeff Nichols, associate lab director for computing and computational sciences at Oak Ridge National Laboratory. HPE and AMD are currently developing another exascale supercomputer called El Capitan for the Department of Energy. The system is expected to be even faster than Frontier. When it launches in 2023, El Capitan will provide more than 2 exaflops of performance.", "url": "https://siliconangle.com/2022/05/30/amd-powered-frontier-supercomputer-ranked-fastest-world/"}, {"title": "US Takes Supercomputer Top Spot With First True Exascale Machine", "date": "2022-05-30", "content": "The world's fastest supercomputer resides at the Department of Energy's Oak Ridge National Laboratory (ORNL) and counts as the first true exascale machine with an HPL score of 1.102 exaflops/second. The Frontier supercomputer was announced as the fastest supercomputer today in the 59th TOP500 list(Opens in a new window). It uses Hewlett Packard Enterprise's (HPE) Cray EX platform, and consists of 74 purpose-built cabinets. Contained within them are a mix of AMD EPYC 64C 2GHz processors and AMD Instinct 250X professional GPUs. In total, there are more than 9,400 CPUs and 37,000 GPUs for a total core count of 8,730,112. The huge amount of processing performance achieved equates to 52.23 gigaflops/watt and more than 1 quintillion calculations per second. That's combined with 700 petabytes of storage and HPE Slingshot high-performance Ethernet for data transfers. In order to cool the system, HPE pumps 6,000 gallons of water through Frontier's cabinets every minute using four 350-horsepower pumps. To put this performance leap in context, the previous fastest supercomputer is the Fugaku system installed at the RIKEN Center for Computational Science (R-CCS) in Kobe, Japan. It contains 7,630,848 cores and has a HPL benchmark of just 442 petaflops/second compared to Frontier's 1.1 exaflops/second. Fugaku also offers nearly three times the processing power of the supercomputer in third place.", "url": "https://www.pcmag.com/news/us-takes-supercomputer-top-spot-with-first-true-exascale-machine?utm_source=email&utm_campaign=whatsnewnow&utm_medium=title"}, {"title": "Department of Energy ready to use Frontier supercomputer to solve 24 science problems", "date": "2022-06-06", "content": "The Department of Energy is poised to use its Frontier supercomputer to tackle 24 initial science and engineering problems with its applications and software stack. The system, which is a keystone of the agency's Exascale Computing Project, was late last month declared - albeit with some caveats - to have retaken the position as the world's fastest exascale system. The Department of Energy now plans to scale to about 9,400 nodes - separate computers that make up a high-performance computing cluster, processing at a speed of 1,880 petaflops - of Oak Ridge Leadership Computing Facility's exascale computer to simulate problems of national interest over the next 18 months. Exascale Computing Project has already been running on about 200 nodes of Frontier hardware since January, with one-and-a-half cabinets of the exascale computer's 74 having been set aside for development teams to prepare for and identify last-minute issues with the transition. Advertisement \"It's crunch time, and we're very excited,\" Doug Kothe, director of the ECP, told FedScoop. \"Within the next couple of months we're going to be getting on Frontier and demonstrating very specific application capabilities and performance capabilities.\" Working with DOE sponsors, ECP chose the 24 initial science and engineering problems to focus on, though future possibilities number in the hundreds or thousands, Kothe said. Problems span key national pillars: economic, national and energy security; scientific discovery; and even health care - so long as they're exascale problems. \"An exascale problem is one that was really not solvable or approachable without this kind of power,\" Kothe said. \"It might mean that my concept-to- design cycle is a certain period, and to be able to simulate some phenomena in a design - with all the complexity I need to make a good solution - takes much longer than that design cycle.\" Engineers want results in days, hours or minutes, so months-long simulations aren't practical. Exascale computers can simulate more complex phenomena with higher confidence while still allowing for much quicker hypothesis cycles. Advertisement Energy production is a core focus for DOE, so ECP will use Frontier to simulate fusion and nuclear fission reactors, wind energy farms, power grids, the clean combustion of fossil fuels like coal, and internal combustion engines used by land-based turbines and gas power plants. ECP has a power grid app for energy transmission and hopes to simulate a large portion of the nation's interconnections. ECP will also use Frontier to answer fundamental science questions around the origin of the elements in the universe including astrophysics, neutron star mergers and supernovae; the evolution of the universe known as cosmology; and the fundamental forces of nature like quantum chromodynamics. Stanford Linear Accelerator Laboratory has an ECP project simulating how light sources interact with matter by shining photons from a free electron laser through biological samples or metal alloys to understand their structure. Nontraditional apps include simulating genome assembly for microbiomes, how materials respond in extreme conditions like a radiation environment and COVID-19 virus docking scenarios. \"I'm pretty confident that we're going to see some major discoveries come out of this exascale era with the applications that we've developed,\" Kothe said. Advertisement ECP is part of the broader National Strategic Computing Initiative begun by DOE in 2016, and its apps and software aren't exascale-specific - meaning they run on laptops, desktops and clusters too. The Extreme Scale Scientific Software Stack, released more than three years ago at E4S.io, is comprised of the libraries apps need to come up with solutions and sits atop a Hewlett Packard Enterprise operating system. DOE leads NSCI, and its Industry and Agency Council targets five agencies: the Department of Defense, National Institutes of Health, National Science Foundation, National Oceanic and Atmospheric Administration, and NASA. DOE is working with NIH to develop machine learning to address cancer, called the Cancer Distributed Learning Environment, using Frontier. Frontier apps may be built on multiple physical phenomena - fluid flow, heat transfer and material science - exportable to apps used by agencies outside the big five. Advertisement Developing Frontier was a codesign effort between ECP and the Oak Ridge Leadership Computing Facility, as is the case with the Aurora and El Capitan exascale computers at the Argonne and Lawrence Livermore national labs, respectively. ECP set requirements for the systems. \"We understand very well the hardware that's being deployed,\" Kothe said. \"And we're tailoring our applications to run well and exploit that hardware.\"", "url": "https://fedscoop.com/exascale-computing-project-frontier-24-problems/"}, {"title": "PUSHING THE NEW FRONTIER", "date": "2022-06-08", "content": "The numbers are in, Frontier ranks No. 1, and the researchers of Oak Ridge National Laboratory's National Center for Computational Sciences are finally breathing a few sighs of relief. Teams worked around the clock for months to help prepare the new HPE Cray EX supercomputer and gauge its record-setting exascale performance. Frontier debuted at the end of May as the world's fastest supercomputer on the 59th Top500 list at 1.1 exaflops - more than 1 quintillion double-precision calculations per second. The system, housed at the Oak Ridge Leadership Computing Facility, topped the international rankings not only for overall computing speed but in a newer ranking for AI speeds at more than 6.86 quintillion mixed-precision calculations per second and for energy efficiency at 62.8 gigaflops per watt. To measure Frontier's AI capabilities, NCCS researchers in the Analytics and AI Methods at Scale Group developed a code in-house with help from colleagues in the Algorithms and Performance Analysis Group that could gauge the mixed-precision calculations typically relied on by AI networks. This code, known as an HPL-AI benchmark, represents a new capability for ORNL. The team first ran the benchmark code at scale on Summit, Frontier's 200-petaflop predecessor, to demonstrate its algorithmic correctness and scalability. Then came the time to run the code on Frontier. \"There are a lot of complexities, optimization paths and trade-offs associated with driving such a benchmark to the maximum at this unprecedented scale,\" said Feiyi Wang, the group's leader. \"Vendors typically develop and run these benchmarks themselves and treat them as industry secrets. We took a risk, as this is something we have never done before. Thanks to the relentless drive for excellence from our staff scientists at the Lab, we did it.\" Hao Lu, a research scientist, and Michael Matheson, a visualization specialist, took the lead on developing the new capability, and the entire group pitched in. The code they produced works across two major computing platforms to set a new high point for benchmarking supercomputer performance. The team plans to publish the code to aid other researchers worldwide. \"It's the first known code base of this kind that can run on both of the most important accelerator-computing platforms,\" Lu said. Next steps include steadily and symmetrically monitoring and profiling Frontier's performance to reach even greater speeds. \"We developed our own code base not just for the sake of producing a single performance number, and that single number doesn't and can't tell the whole story,\" Wang said. \"To some degree, you can say all benchmark numbers are wrong, but some are useful. This development will not only help us establish a performance baseline, but also position us better to shape this and future next-generation leadership computing systems from acquisition and evaluation all the way to acceptance testing. Moreover, we have demonstrated a variety of optimization techniques that can be broadly useful and applicable to a wide spectrum of scientific applications. \"We couldn't have done this without such ardent support and continuous encouragement from NCCS leadership on taking risks. There have been many unsung heroes and of course, a truly great team responding to the mission calls and striving for excellence. We are fortunate to play our part in this significant milestone project.\"", "url": "https://www.olcf.ornl.gov/2022/06/08/pushing-the-new-frontier/"}, {"title": "The Beating Heart of the World's First Exascale Supercomputer", "date": "2022-06-24", "content": "The world's latest fastest supercomputer, Frontier at Oak Ridge National Lab, in Tennessee, is so powerful that it operates faster than the next seven best supercomputers combined and more than twice as well as the No. 2 machine. Frontier is not only the first machine to break the exascale barrier, a threshold of a billion billion calculations per second, but is also ranked No. 1 as the world's most energy-efficient supercomputer. Now the companies that helped build Frontier, Advanced Micro Devices and Hewlett Packard Enterprise, reveal the electronic tricks that make the supercomputer tick. Frontier consists of 74 HPE Cray EX supercomputing cabinets, each weighing more than 3,600 kilograms, which altogether hold more than 9,400 computing nodes. Each node contains one optimized third- generation AMD EPYC 64-core 2-gigahertz \"Trento\" processor for general tasks and four AMD instinct MI250x accelerators for highly parallel supercomputing and AI operations, as well as 4 terabytes of flash memory to help quickly feed the GPUs data. In total, Frontier contains 9,408 CPUs, 37,632 GPUs, and 8,730,112 cores, linked together by 145 kilometers of networking cables. The lab says its world- leading supercomputer consumes about 21 megawatts. In May at the International Supercomputing Conference 2022 in Hamburg, Frontier revealed an overall performance of 1.1 exaflops, or 1.1 quintillion floating point operations per second, launching it to head of the Top500 list of the world's most powerful supercomputers. It may grow even more powerful, with a theoretical peak performance of 2 exaflops. In addition, Frontier is ranked first on the latest Green500 list, which measures supercomputing energy efficiency. Whereas the previous top Green500 machine, MN-3 in Japan, delivered 39.38 gigaflops per watt, the Frontier test-and development system achieves 62.68 gigaflops per watt. Moreover, Frontier won the top spot in a newer category, mixed-precision computing, which rates performance in computing formats commonly used for artificial intelligence. On the latest High-Performance Linpack-Accelerator Introspection or HPL-AI test, Frontier's performance reached about 6.86 exaflops. A key aspect of Frontier's success is how its CPUs and GPUs are linked within each node via AMD's Infinity Fabric interconnect architecture. This helps boost coherency between the CPU and GPUs-that is, giving them all the same view of shared data. \"Coherency is very important to getting you to scale performance,\" says Brad McCredie, corporate vice president of data center GPU and accelerated processing at AMD in Austin. \"It helps you make sure that you can run the right workloads on the right processors. It makes it very easy for CPUs to do small pieces of work and GPUs to do big pieces of work in parallel.\" During Frontier's development, AMD noted the biggest challenge it faced was power performance. \"There was a lot of documentation that it would take hundreds of thousands of GPUs and 150 to 500 MW to get to an exaflop, and we wanted to do it with tens of thousands of GPUs and 20 MW\" McCredie says. \"So everyone up and down the line went after efficiency.\" For example, Frontier's GPUs each have 128 gigabytes of high-bandwidth memory soldered onto them. This helps them overcome a critical bottleneck to performance-the shuffling of data between memory and processing. Moreover, Frontier's GPUs each used the advanced 6-nanometer node from TSMC. Therefore, \"they can execute double-precision floating-point operations as fast as single-precision floating-point operations, which was a big innovation,\" McCredie says. These seemingly inconsequential developments in fact helped Frontier rely on tens of thousands of GPUs rather than hundreds of thousands, \"shifting the burden away from the programmer to the hardware when it comes to managing all that parallelism,\" McCredie says. \"That makes the system much more programmable.\" Two AMD nodes fit on a \"compute blade,\" and 64 such blades are loaded into each cabinet. The compute blades are linked together by HPE Slingshot interconnects, each with a custom-designed 64-port switch that provides 12.8 terabits per second of network bandwidth. Groups of blades are linked together via a so-called dragonfly topology in which hundreds of cabinets with hundreds of thousands of nodes can all communicate with just three hops at most between all nodes. \"Slingshot deployments are highly optimized to use the most energy-efficient cabling-direct attach copper and active optical cables- fitted to the distances required,\" says Mike Woodacre, vice president and chief technical officer of HPE's HPC and AI team. Eliminating less-efficient general-purpose components, he adds, \"significantly reduces the energy the fabric consumes.\" The blades in the cabinets are chilled using liquid cooling. According to Gerald Kleyn, vice president of HPC and AI systems at HPE, the supercomputer can achieve up to five times the density of a traditional, air-cooled architecture. The result is a compact system that in turn dramatically reduces cabling requirements and operational expenses. \"Breaking the exaflop barrier was important, but doing so while achieving No. 1 on the Green500 list is remarkable,\" says Kleyn. Moreover, accomplishing this in the midst of a pandemic and global supply-chain problems, he says, \"took a herculean team effort between Oak Ridge National Laboratory, HPE, and AMD.\" The next steps for Frontier include continued testing and validation of the system. The lab says it remains on track for final acceptance and early science access later in 2022 and is planned to open for full science at the beginning of 2023. Projects already planned for Frontier include research into cancer, drug discovery, nuclear fusion, exotic materials, superefficient engines, and stellar explosions. The aim of the machine is to speed the time required for such work from weeks to hours and from hours to seconds. \"Frontier enable scientists to do more science, which means getting closer to more efficient cleaner-burning energy, more quickly finding even more effective vaccines for viruses,\" McCredie says. \"We started this whole adventure with Frontier to be the first to an exaflop, but seeing people at Oak Ridge working to solve problems in climate, energy, the pandemic, the top challenges facing humanity-we've gone from wanting to build a powerful computer to building something that will help everyone.\"", "url": "https://spectrum.ieee.org/frontier-exascale-supercomputer"}]}, {"win label": 0, "news": [{"title": "Exascale Supercomputer \"Frontier\" For The U.S. Department Of Energy", "date": "2022-07-24", "content": "Hewlett Packard Enterprise announced that Frontier, a new supercomputer that it built for the U.S. Department of Energy's Oak Ridge National Laboratory, has reached 1.1 exaflops, making it the world's first supercomputer to break the exascale speed barrier, and the world's fastest supercomputer, according to the Top500 list of world's most powerful supercomputers. Frontier also ranked number one in a category, called mixed-precision computing, that rates performance in formats commonly used for artificial intelligence, with a performance of 6.88 exaflops. Additionally, the new supercomputer claimed the number one spot on the Green500 list as the world's most energy efficient supercomputer with 52.23 gigaflops performance per watt, making it 32 percent more energy efficient compared to the previous number one system. In addition to Frontier, three more HPE-built systems are named to the top 10 of the Top500 list, including the LUMI supercomputer for the CSC - IT Center for Science in Finland at number three, Perlmutter supercomputer for the U.S. Department of Energy's National Energy Research Scientific Computing Center at number seven, and the Adastra supercomputer for GENCI- CINES at number ten. As the most powerful supercomputer in the world, delivering unprecedented performance and advanced capabilities, Frontier will speed up discoveries, make breakthroughs, and address the world's toughest challenges. The supercomputer, which is more powerful than the next top seven of the world's largest supercomputers, will allow scientists to model and simulate at an exascale level to solve problems that are eight times more complex, up to ten times faster. Frontier is also expected to reach even higher levels of speed with a theoretical peak performance of 2 exaflops. The supercomputer will have significant impact in critical areas such as cancer and disease diagnosis and prognosis, drug discovery, renewable energy, and new materials to create safer and sustainable products. ", "url": "https://e3zine.com/exascale-supercomputer-frontier-for-the-u-s-department-of-energy/"}, {"title": "ORNL CELEBRATES LAUNCH OF FRONTIER - THE WORLD'S FASTEST SUPERCOMPUTER", "date": "2022-08-17", "content": "The U.S. Department of Energy's Oak Ridge National Laboratory celebrated the debut of Frontier, the world's fastest supercomputer and the dawn of the exascale computing era. Deputy Secretary of Energy David Turk, DOE Office of Science Director Asmeret Asefaw Berhe and U.S. Rep. Chuck Fleischmann joined ORNL Director Thomas Zacharia, ORNL Site Office Director Johnny Moore and computing vendor partners Lisa Su, chair and chief executive officer of AMD, and Antonio Neri, president and CEO of HPE, to congratulate the public-private team that made Frontier's record-setting performance possible. \"Research that might once have taken weeks to complete, Frontier will tear through in hours, even seconds,\" Turk said. \"Oak Ridge has positioned the United States to lead the world in solving massive scientific challenges across the board.\" \"Exascale computing is a powerful tool that will allow us to advance the core missions of the Office of Science - to deliver scientific discoveries and major scientific tools that will transform our understanding of nature and advance the energy, economic, and national security of the U.S.,\" Berhe said. \"Frontier makes exascale computing a reality and opens many doors for the future of scientific research to solve big problems.\" Frontier leverages ORNL's extensive expertise in accelerated computing for open science and will enable researchers to tackle problems of national and global importance deemed impossible to solve as recently as five years ago. \"We are incredibly proud of the team that has made ORNL home to the world's first exascale computer. This accomplishment was possible due to the strong public-private partnerships between DOE, ORNL, HPE, and AMD,\" Zacharia said. \"Working with our sister labs and academic partners, Frontier is already delivering science on day one.\" Frontier earned the No. 1 spot on the 59th TOP500 list in May 2022 with 1.1 exaflops of performance - more than a quintillion, or 1018, calculations per second - making it the fastest computer in the world and the first to achieve exascale. \"As the world's most powerful AI machine, Frontier's novel architecture is also ideally suited for delivering unprecedented machine learning and data science insights and automations that could vastly improve our understanding of critical processes, from drug delivery to nuclear fusion to the global climate,\" said Doug Kothe, associate laboratory director of ORNL's Computing and Computational Sciences Directorate and director of the Exascale Computing Project. \"Frontier marks the start of the exascale era for scientific computing,\" said Bronson Messer, director of science for ORNL's Oak Ridge Leadership Computing Facility, which houses Frontier. \"The science that's going to be done on Frontier is going to ignite an explosion of innovation - and of new questions we haven't even thought of before.\" The new machine also claimed the top spot on the Green500 list, which rates a supercomputer's energy efficiency in terms of performance per watt. Frontier clocked in at 62.68 gigaflops, or nearly 63 billion calculations, per watt. Frontier also holds the top ranking in the new mixed-precision computing benchmark that rates performance in arithmetic precisions commonly used for artificial intelligence problems. \"This is a very important milestone for the nation and the world,\" said Gina Tourassi, director of ORNL's National Center for Computational Sciences, which oversees the OLCF. \"The computational models we can build with this computer will help us fill in missing pieces of the puzzle for a range of scientific inquiries, from matter and energy to life itself, and will give the next generation of scientists the tools and the springboard they need to make even greater leaps of understanding.\" ORNL's scientific partners, such as General Electric Aviation and GE Power, plan to leverage the power of Frontier to revolutionize the future of flight with sustainable hydrogen propulsion and hybrid electric technologies and to maximize the potential of clean-energy technologies such as wind power. \"GE Aerospace and Research will be using exascale computing, including time on the Frontier supercomputer, to revolutionize the future of flight with sustainable hydrogen propulsion and hybrid electric technologies,\" said David Kepczynski, chief information officer at GE Research. \"In pursuit of a net-zero carbon future, exascale supercomputing systems will be indispensable tools for GE researchers and engineers working at the cutting edge to 'Build a World that Works.'\" The work to deliver, install and test Frontier began in the midst of the COVID-19 pandemic, as shutdowns around the world strained international supply chains. More than 100 team members worked around the clock to source millions of components, ensure timely deliveries of system parts, and carefully install and test 74 HPE Cray EX cabinets that include more than 9,400 AMD-powered nodes and 90 miles of interconnect cables. \"Frontier is a landmark in computing that will usher in a new era of insights and innovation,\" said Antonio Neri, president and CEO of HPE. \"We are proud of this massive achievement that will help make significant contributions to science, push the envelope for artificial intelligence, and strengthen U.S. industrial competitiveness. Frontier was made possible through powerful engineering and design, and most importantly, through a strong partnership between Oak Ridge National Laboratory, HPE and AMD.\" Each of Frontier's more than 9,400 nodes is equipped with a third-generation AMD EPYC CPU and four AMD Instinct MI250X graphic processing units, or GPUs. Combining traditional CPUs with GPUs to accelerate the performance of leadership-class scientific supercomputers is indicative of the hybrid computing paradigm pioneered by ORNL and its partners. \"At its heart, Frontier highlights the importance of long-term public private partnerships and the important role high performance computing plays advancing scientific research and national security,\" said Lisa Su, chair and CEO of AMD. \"I am excited to see Frontier enable large scale science research that was previously not possible, leading to new discoveries in physics, medicine, climate research and energy that will transform our daily lives.\" Frontier's deployment adds to ORNL's nearly 20-year tradition of supercomputing excellence alongside predecessors Jaguar, Titan and Summit - each the world's fastest computer in its time. \"This project marks the culmination of more than three years of effort by hundreds of dedicated ORNL professionals and their counterparts at HPE and AMD and across the DOE community,\" said Justin Whitt, director of the OLCF. \"Their hard work will enable scientists around the world to begin their explorations on Frontier. At the OLCF, we're proud of our legacy of world-leading computer excellence.\" ORNL and its partners are on schedule as they continue the stand-up of Frontier. Next steps include additional testing and validation of the system, which remains on track for final acceptance and early science access later in 2022. Full access for science applications is expected at the beginning of 2023.", "url": "https://www.olcf.ornl.gov/2022/08/17/ornl-celebrates-launch-of-frontier-the-worlds-fastest-supercomputer/"}, {"title": "Frontier Supercomputer Breaks Exascale Barrier", "date": "2022-09-12", "content": "The world's first and fastest supercomputer, Frontier, built by HPE for the U.S. Department of Energy's Oak Ridge National Laboratory (ORNL), has reached 1.1 exaflops, breaking the exascale speed barrier (a threshold of a quintillion calculations per second), and ranking number one on the Top500 list of world's most powerful supercomputers. Notably, it is ranked as the world's most energy-efficient supercomputer on the Green500 list, which rates energy use and efficiency by commercially available supercomputing systems, with Frontier delivering 62.68 gigaflops performance per watt. Frontier features a theoretical peak performance of 2 exaflops, or two quintillion calculations per second, making it ten times more powerful than ORNL's Summit system. It consists of 74 HPE Cray EX supercomputer cabinets, which include more than 9,400 AMD-powered nodes and 90 miles of networking cables. Each node contains one optimized EPYC\u2122 processor and four AMD Instinct\u2122 accelerators for more than 9,400 CPUs and more than 37,000 GPUs in the entire system. Check Figure 1 for other fun facts about Frontier.", "url": "https://climatemodeling.science.energy.gov/news/frontier-supercomputer-breaks-exascale-barrier"}, {"title": "Frontier supercomputer suffering 'daily hardware failures' during testing", "date": "2022-10-10", "content": "Oak Ridge National Laboratory's (ORNL) upcoming exascale Frontier supercomputer is seeing daily hardware failures during its testing phase. First announced in 2019, Frontier is based on Cray's new Shasta architecture and Slingshot interconnect, and features upcoming AMD Epyc CPUs and Radeon Instinct GPUs. The system is expected to provide 1.5 exaflops of performance once fully operational and available to researchers. The system is officially the world's fastest supercomputer, and the first to break the exascale barrier - though China is thought to be running a number of exascale systems it hasn't entered onto the Top500 list. InsideHPC reports the system's current problems appear to center on Frontier's stability when executing highly demanding workloads, with some of the problems focused on AMD's Instinct GPU accelerators, which carry most of the system's processing workload and are paired with AMD Epyc CPUs within the system's blades. The publication has previously reported on problems with Frontier's HPE Cray Slingshot fabric lasting from late last year into the spring of this year. Justin Whitt, program director for the Oak Ridge Leadership Computing Facility (OLCF), said the issues are typical of those previously seen during the testing and tuning of supercomputers at the lab. \"We are working through issues in hardware and making sure that we understand (what they are) because you're going to have failures at this scale,\" he said. \"Mean time between failure on a system this size is hours, it's not days, so you need to make sure you understand what those failures are and that there's no pattern to those failures that you need to be concerned with.\" \"It's mostly issues of scale coupled with the breadth of applications, so the issues we're encountering mostly relate to running very, very large jobs using the entire system ... and getting all the hardware to work in concert to do that,\" Whitt added. \"That's kind of the final exam for supercomputers. It's the hardest part to reach.\" A day-long run without a system failure \"would be outstanding,\" Whitt said. \"Our goal is still hours\" but longer than Frontier's current failure rate, adding that \"we're not super far off our goal. The issues span lots of different categories, the GPUs are just one.\" \"I don't think that at this point that we have a lot of concern over the AMD products. We're dealing with a lot of the early-life kind of things we've seen with other machines that we've deployed, so it's nothing too out of the ordinary.\" The supercomputer consists of 74 cabinets, each weighing in at 8,000 pounds. They feature 9,408 HPE Cray EX nodes, each of which has a single AMD 'Trento' 7A53 Epyc CPU and four AMD Instinct MI250X GPUs, for a total of 37,632 GPUs. Across the system, it has 8,730,112 cores. The supercomputer spans 372 square meters (4,004 sq ft) and consumes 40MW of power at peak. Frontier will be followed by Aurora, which also began installation late last year. Featuring Intel's Ponte Vecchio GPU, it is expected to be capable of 2 exaflops. Another 2 exaflops system is expected next year, the AMD-powered El Capitan supercomputer.", "url": "https://www.datacenterdynamics.com/en/news/frontier-supercomputer-suffering-daily-hardware-failures-during-testing/"}, {"title": "FAST-TRACKING MEDICAL DISCOVERY", "date": "2022-10-25", "content": "The world's fastest supercomputer could help discover the next great cure hiding in plain sight. Researchers at the U.S. Department of Energy's Oak Ridge National Laboratory used Frontier, the world's first exascale computer, to scan hundreds of thousands of biomedical concepts from millions of scientific publications in search of potential connections among symptoms, diseases, conditions and treatments. The effort began as part of the fight against COVID-19 but could eventually become as essential to basic diagnosis and treatment as Google to an online search. \"We're connecting dots at high speed,\" said Ramakrishnan \"Ramki\" Kannan, the study's lead author and an ORNL computational scientist. \"We want to take these various medical concepts, connect them and show how they relate to each other. The results still have to be confirmed but could help us understand relationships between everything from allergies to diseases like cancer, malaria and COVID-19 to find unexpected solutions.\" The study earned the team a finalist nomination for the Association of Computing Machinery Gordon Bell Prize. The prize, awarded annually since 1987, recognizes outstanding achievements in applying high-performance computing to challenges in science, engineering and large-scale data analytics. This year's winners will be presented at the International Conference for High-Performance Computing, Networking, Storage and Analysis, set for Nov. 13-18, 2022, in Dallas. The team's project seeks to push the fast-forward button on drug discovery to streamline exploration for promising leads. The inspiration came in early 2020 during the COVID-19 pandemic, when scientists around the world turned their attention to searching for potential treatments. Kannan and fellow scientist Tom Potok of ORNL's Computer Science and Mathematics Division led a team of researchers from ORNL, AMD, the Georgia Institute of Technology and the University of California, San Francisco that developed the Distributed Accelerating Semiring All-Pairs Shortest Path algorithm, or DSNAPSHOT, a method using AI to pinpoint potential links amid millions of medical concepts across decades of scientific publications. The team made the algorithm's target a dataset on COVID-19 and associated coronaviruses, drawn from more than 800,000 papers. \"Whenever humanity faces such a formidable challenge, the first step we always take is to stand on the shoulders of giants,\" Kannan said. \"What do we already know, and where can it lead us? Let's take these publications and connect the concepts. For example: Patients with COVID-19 often have fevers. Can a drug that treats other kinds of fever help treat this disease? Is there a drug already used for cancer that may help treat COVID-19? How can we find likely connections and gauge which may be the most promising?\" The team used DSNAPSHOT and another algorithm - the Communication-Optimized All-Pairs Shortest Path, or COAST - to plot each concept already identified by scientists as a point, or vertex, across a graph and draw virtual paths or \"edges\" between the various points. They sought to expand the digital dragnet from that initial dataset to a graph of concepts pulled from the U.S. National Library of Medicine's PubMed database, using Summit, Frontier's predecessor and then the nation's fastest supercomputer at 200 petaflops, to power the search. \"Some of these connections will be obvious, some will be unworkable, and some will be promising,\" Potok said. \"Can we narrow the results to what's promising? Searches like this could take decades using a standard computer. We wanted to try to shrink that time to hours or minutes.\" Even Summit could process only about a sixth of the graph, which spanned more than 35 million PubMed citations. The team then turned to Frontier, fresh from its spring 2022 debut as the world's fastest supercomputer at 1.1 exaflops, or more than 1 quintillion calculations per second. \"Give a calculator to each of the world's 7 billion people, ask them to perform one calculation per second, and it would take them nearly five years to do what Frontier can do in a single second,\" Kannan said. \"We knew we needed a next-generation computer of this caliber to achieve what we wanted. Frontier solved the problem we couldn't solve on Summit.\" The team used 9,200 of Frontier's more than 9,400 nodes to perform an initial search across a graph drawn from PubMed and the Scalable Precision Medicine Open Knowledge Engine, or SPOKE, a comprehensive index of medical databases maintained by UC San Francisco. The run reached a speed of 1 exaflop at single precision and took only 11.7 minutes to search more than 7 million data points drawn from 18 million publications. \"We identified four sets of paths,\" Kannan said. \"The next steps require further studies such as clinical trials to validate.\" The team ultimately hopes to scale up the algorithm to scan the full depth of SPOKE and PubMed combined and to make the search as easily customizable as a Google query. \"There may be connections we would never discover otherwise,\" Kannan said. \"We want to index and understand the relationships between all of these - the diseases, symptoms, treatments, complications - so during the next pandemic, we can have potential answers closer at hand.\" Support for this research came from the DOE Office of Science's Advanced Scientific Computing Research program. The OLCF is a DOE Office of Science user facility at ORNL.", "url": "https://www.olcf.ornl.gov/2022/10/25/fast-tracking-medical-discovery/"}, {"title": "Frontier Keeps Top Supercomputer Spot, Nvidia's H100 Debuts on List", "date": "2022-11-14", "content": "OThe 60th edition of the Top500 list, revealed today at SC22 in Dallas, Texas, showcases many of the same systems as the previous installment, with Frontier still out in front as the first official Linpack exascaler, clocking 1.102 exaflops. Installed at Oak Ridge National Laboratory, Frontier - a collaboration of the DOE, HPE and AMD - comprises 74 HPE Cray EX cabinets, housing 9,408 nodes, each equipped with one AMD Milan \"Trento\" Epyc CPU and four AMD Instinct MI250X GPUs. Frontier also scored highest on the HPL-MxP benchmark with 7.9 exaflops. A companion benchmark to the Top500, HPL-MxP was formerly known as HPL-AI. The benchmark \"seeks to highlight the convergence of HPC and artificial intelligence (AI) workloads based on machine learning and deep learning by solving a system of linear equations using novel, mixed-precision algorithms that exploit modern hardware,\" according to the backers. Lumi (#3 Top500 system) had the second highest HPL-MxP with 2.2 exaflops, followed by the Fugaku supercomputer (still in 2nd place on the Top500) which moved from its previous number one spot into third position with 2.0 exaflops. Fugaku retained its number one spot on the HPCG (High Performance Conjugate Gradients) benchmark with 16 petaflop, 3% out of peak. Frontier came in second place with a score of 14 petaflops, which was only .8 percent of peak. \"That low efficiency to peak is an indication that the machine struggles with data movement compared to floating point,\" said Jack Dongarra, Top500 coauthor and progenitor of the High Performance Linpack benchmark, in an interview with HPCwire. \"While HPL is compute-intensive, HPCG is a data movement benchmark. It's trying to capture something about 3D partial differential equations,\" said Dongarra. \"[These systems] don't do data movement as well as they should. I think [that is] highlighting the weakness of these machines and allowing us to, I hope, use that information in designing the next generation.\" He gives credit to the projects that are participating in the HPCG project. \"You can portray this like a race car. If you had a race car that was capable of going 200 miles an hour, and then it gets two miles an hour, you're not gonna be very happy with that rate.\" There is only one new entrant in the top 10 cohort: EuroHPC's Leonardo supercomputer, which landed on the list in fourth position with 174.70 Linpack petaflops out of 255.75 theoretical peak petaflops. The Italian system, housed at CINECA and built by Atos, comprises more than 3,000 Nvidia HGX nodes, each equipped with four Nvidia A100 GPUs (the 40 GB variant), and a single Intel Ice Lake CPU. The water-cooled nodes are connected via Nvidia Mellanox HDR 200Gb/s InfiniBand networking. Leonardo is the sixth EuroHPC system to be stood up. Two are outstanding: Deucalion, a Fujitsu-Arm system to be sited in Portugal, had been expected by the end of this year, and MareNostrum-5 - a Lenovo-Atos collaboration with the Barcelona Supercomputing Center - which is said to be coming next year. As expected, Finnish supercomputer Lumi significantly expanded its computational footprint, doubling its previous score from 151.9 petaflops to 309.1 Linpack petaflops. Lumi boasts a green datacenter, thanks to its use of 100 percent renewable energy (hydropower). Its waste heat will be used to supply approximately 20 percent of the yearly district heating needs of its host town, resulting in a stated net negative carbon footprint of 13,500 tons of CO2 equivalent per year. Green500 Welcomes 'Henri': The Green500, meanwhile, is host to an even bigger showing from Frontier-style systems. May's Green500 list had four such systems in the first four places: respectively, Frontier TDS (the test and development system for Frontier); Frontier itself; EuroHPC's LUMI system; and GENCI's Adastra system. All four of those HPE-built systems used AMD Epyc \"Milan\" CPUs, AMD Instinct MI250X GPUs and Slingshot 11 networking in the same configuration. This year, those four systems are joined by the similarly-architectured GPU partition of Pawsey's Setonix system and the GPU partition of KTH's Dardel system. Collectively, those systems - in order, Frontier TDS, Adastra, Setonix GPU, Dardel GPU, Frontier and LUMI - now occupy 2nd place through 7th place on the November 2022 Green500 list. The French system Adastra (an HPE-AMD system) increased its energy-efficiency to 58.02 gigaflops per watt, moving up one spot into third position. Adastra ranks #11 on the new Top500 list with a Linpack score of 46.10 petaflops. In first place: a shot across the bow from Nvidia's H100 GPU, courtesy of a small system named Henri at the Flatiron Institute in New York. Henri holds a #405 ranking on the Top500. Henri provides just 2.04 Linpack petaflops, out of a theoretical peak of 5.42 petaflops, giving it an unusually low Linpack efficiency of 37.6%. Henri boasts a whopping 65.1 gigaflops per watt, besting Frontier TDS' 62.7 gigaflops per watt. It's an interesting showing from Nvidia, who found themselves thoroughly ousted from a few of the top Green500 spots when the Frontier-style nodes took the stage. Henri is based on Lenovo's ThinkSystem SR670 V2 platform, with Intel Xeon Platinum 8362 2800Mhz (32-cores), the aforementioned H100 80GB PCIe cards and InfiniBand HDR. \"It looks like this was rushed so they could pass the test,\" said Jack Dongarra in an interview. \"Machines usually get between 70 and 85% of the peak, that's respectable for HPL. So when you score so low, something's not right. But it should leave room for further optimizations.\" Recall too that the H100 here is being paired with the older Ice Lake chips. The full capability of the platform won't be available until it is paired with either a (forthcoming) Intel Sapphire Rapids or a (now-shipping) AMD Genoa CPU, which have support for PCIe Gen5 and CXL.", "url": "https://www.hpcwire.com/2022/11/14/frontier-keeps-top-supercomputer-spot-nvidias-h100-debuts-on-list/"}]}]